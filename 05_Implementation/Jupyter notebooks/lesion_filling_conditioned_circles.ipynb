{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcd1318-8fee-4938-b366-05698a04bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './custom_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77b51cd-dfaa-44b4-b1f6-a36e07016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1 \n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "    num_epochs = 280 #600\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 100 #500\n",
    "    evaluate_epochs = 20 #30\n",
    "    evaluate_num_batches = 20 # one batch needs ~15s.  \n",
    "    evaluate_3D_epochs = 1000  # one 3D evaluation needs ~20min\n",
    "    save_model_epochs = 60 # 300\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-cond-circle\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./datasets/filling/dataset_train/imgs\"\n",
    "    segm_train_path = \"./datasets/filling/dataset_train/segm\"\n",
    "    masks_train_path = \"./datasets/filling/dataset_train/masks\"\n",
    "    dataset_eval_path = \"./datasets/filling/dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./datasets/filling/dataset_eval/segm\"\n",
    "    masks_eval_path = \"./datasets/filling/dataset_eval/masks\" \n",
    "    train_only_connected_masks=False # No Training with lesion masks\n",
    "    eval_only_connected_masks=False \n",
    "    num_inference_steps=50\n",
    "    mode = \"train\" # train / eval\n",
    "    debug = True\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5a5223-3c9a-4287-b44a-1a6c1a411538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps=1\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1 \n",
    "    config.train_only_connected_masks=False\n",
    "    config.eval_only_connected_masks=False\n",
    "    config.evaluate_num_batches=1\n",
    "    dataset_train_path = \"./dataset_eval/imgs\"\n",
    "    segm_train_path = \"./dataset_eval/segm\"\n",
    "    masks_train_path = \"./dataset_eval/masks\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e2ffca-7e8c-43d4-8e13-ba1900e38eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetMRI2D import DatasetMRI2D\n",
    "from DatasetMRI3D import DatasetMRI3D\n",
    "from pathlib import Path\n",
    "\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI2D(root_dir_img=Path(config.dataset_train_path), root_dir_segm=Path(config.segm_train_path), only_connected_masks=config.train_only_connected_masks)\n",
    "datasetEvaluation = DatasetMRI2D(root_dir_img=Path(config.dataset_eval_path), root_dir_masks=Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "dataset3DEvaluation = DatasetMRI3D(root_dir_img=Path(config.dataset_eval_path), root_dir_masks=Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b10865-fac0-4151-a227-d426c2fd20da",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6286c-155d-4d6c-923d-bef8ce0b180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axis = plt.subplots(1,2,figsize=(16,4))\n",
    "idx=80\n",
    "axis[0].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "axis[1].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "fig.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c57471-c0bf-4bca-9207-4dc6194bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 6 random sample\n",
    "random_indices = np.random.randint(0, len(datasetTrain) - 1, size=(6)) \n",
    "\n",
    "# Plot: t1n segmentations\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6fe06-53ba-4056-bffb-c1c4acecacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: t1n images\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba21352-9edc-4351-a359-de83bbed114d",
   "metadata": {},
   "source": [
    "### Playground for random circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18221bb-ac64-4876-9a87-24a8c0ff7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normal distributions of center points\n",
    "centers=[]\n",
    "for _ in np.arange(100):\n",
    "    centers.append(torch.normal(torch.tensor([127.,127.]),torch.tensor(30.)))\n",
    "\n",
    "plt.imshow((datasetTrain[70][\"gt_image\"].squeeze()+1)/2) \n",
    "for center in centers:\n",
    "    plt.scatter(center[0], center[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6508289-04f3-46c8-bc05-9629d7799e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=torch.zeros((10,256,256)).shape\n",
    "\n",
    "#create circular mask with random center around the center point of the pictures and a radius between 3 and 40 pixels\n",
    "center=np.random.normal([127,127],30, size=(example[0],2))\n",
    "radius=np.random.uniform(low=3,high=40, size=(example[0]))\n",
    "\n",
    "Y, X = np.ogrid[:256, :256] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "dist_from_center = np.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "\n",
    "mask = dist_from_center <= radius # creates mask for pixels which are inside the radius\n",
    "mask = 1-mask\n",
    "\n",
    "plt.imshow(((datasetTrain[70][\"gt_image\"].squeeze()+1)/2)*mask[:,:,4]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099f07-f1ff-46c1-bc83-2e448b80e778",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509adcf-a0b1-4652-a5ca-b398b008dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3, # the number of input channels: 1 for img, 1 for img_voided, 1 for mask\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.model = \"UNet2DModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f67f7-b932-4f73-bdb3-96751ef6c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "#sample_image = datasetTrain[0]['gt_image'].unsqueeze(0)\n",
    "#noise = torch.randn(sample_image.shape)\n",
    "#timesteps = torch.LongTensor([50])\n",
    "#noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "#tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) \n",
    "\n",
    "#Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])\n",
    "\n",
    "\n",
    "config.noise_scheduler = \"DDIMScheduler(num_train_timesteps=1000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c3aa-3a69-4dfa-bbb2-d3c8086fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "\n",
    "config.lr_scheduler = \"cosine_schedule_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b1603-390b-48b7-81fc-6d0bd96dbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingConditional import TrainingConditional\n",
    "from DDIMInpaintPipeline import DDIMInpaintPipeline\n",
    "from Evaluation2DFilling import Evaluation2DFilling\n",
    "from Evaluation3DFilling import Evaluation3DFilling  \n",
    "import PipelineFactories\n",
    "\n",
    "config.conditional_data = \"Circles\"\n",
    "\n",
    "args = {\n",
    "    \"config\": config, \n",
    "    \"model\": model, \n",
    "    \"noise_scheduler\": noise_scheduler, \n",
    "    \"optimizer\": optimizer, \n",
    "    \"lr_scheduler\": lr_scheduler, \n",
    "    \"datasetTrain\": datasetTrain, \n",
    "    \"datasetEvaluation\": datasetEvaluation, \n",
    "    \"dataset3DEvaluation\": dataset3DEvaluation, \n",
    "    \"trainingCircularMasks\": True, \n",
    "    \"evaluation2D\": Evaluation2DFilling,\n",
    "    \"evaluation3D\": Evaluation3DFilling, \n",
    "    \"pipelineFactory\": PipelineFactories.get_ddim_inpaint_pipeline} \n",
    "trainingCircles = TrainingConditional(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4044c2-9483-4457-bd53-51c795c7bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"train\":\n",
    "    trainingCircles.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9106d1-bf7b-4ec0-bd4c-e980051e7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"eval\":\n",
    "    pipeline = DDIMInpaintPipeline.from_pretrained(config.output_dir) \n",
    "    trainingCircles.evaluate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f790341-a14e-44fe-bfce-a88dec97707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf38b5e5-8e55-4e55-b756-4bfd75248e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_filling_conditioned_circles.ipynb to script\n",
      "[NbConvertApp] Writing 8240 bytes to lesion_filling_conditioned_circles.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix\n",
    "!jupyter nbconvert --to script \"lesion_filling_conditioned_circles.ipynb\"\n",
    "filename=\"lesion_filling_conditioned_circles.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
