{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f1e0a8-8f60-4b1b-94b8-8367832dccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig: \n",
    "    t1n_target_shape = None # will transform t1n during preprocessing (computationally expensive)\n",
    "    unet_img_shape = (128,256)\n",
    "    channels = 1\n",
    "    effective_train_batch_size=16 \n",
    "    eval_batch_size = 1 \n",
    "    num_sorted_samples = 3\n",
    "    num_dataloader_workers = 8\n",
    "    num_epochs = 16\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    evaluate_epochs = 1 # adjust to num_epochs\n",
    "    evaluate_num_batches = 2 # ~3s/batch. 2.5 min/Evaluation 3D epoch with all batchesr\n",
    "    evaluate_num_batches_3d = -1 \n",
    "    deactivate3Devaluation = True\n",
    "    evaluate_3D_epochs = 1000  # 3 min/Evaluation 3D \n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-synthesis-3D\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./datasets/synthesis/dataset_train/imgs\"\n",
    "    segm_train_path = \"./datasets/synthesis/dataset_train/segm\"\n",
    "    masks_train_path = \"./datasets/synthesis/dataset_train/masks\"\n",
    "    dataset_eval_path = \"./datasets/synthesis/dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./datasets/synthesis/dataset_eval/segm\"\n",
    "    masks_eval_path = \"./datasets/synthesis/dataset_eval/masks\"\n",
    "    train_only_connected_masks=False  # No Training with lesion masks\n",
    "    eval_only_connected_masks=False \n",
    "    num_inference_steps=50\n",
    "    log_csv = True\n",
    "    add_lesion_technique = \"other_lesions_median\" # 'mean_intensity', 'other_lesions_1stQuantile', 'other_lesions_mean', 'other_lesions_median', 'other_lesions_3rdQuantile', 'other_lesions_99Quantile'\n",
    "    add_lesion_mean_intensity = -0.5492 \n",
    "    intermediate_timestep = 3 # starting from this timesteps. num_inference_steps means the whole pipeline and 1 the last step. \n",
    "    mode = \"train\" # 'train', 'eval' or \"tuning_parameters\"\n",
    "    debug = True     \n",
    "    jump_length=intermediate_timestep\n",
    "    jump_n_sample=3\n",
    "    brightness_augmentation = False\n",
    "    eval_loss_timesteps=[20,40,80,140]\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub \n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44c81a41-088f-4093-b887-b21579d0c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jovyan/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)\n",
    "#if there are problems with ports then add manually \"main_process_port: 0\" or another number to yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef97359-932d-42b6-b5e0-99823945b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "with open(Path.home() / \".cache/huggingface/accelerate/default_config.yaml\") as f:\n",
    "    data = json.load(f)\n",
    "    config.num_processes = data[\"num_processes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "645585a4-93b9-42b4-b942-7259f02202bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_batch_size = int((config.effective_train_batch_size / config.gradient_accumulation_steps) / config.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc472c2-c374-4859-8ed5-791ebeafbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps = 1\n",
    "    config.intermediate_timestep = 1\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1\n",
    "    config.eval_loss_timesteps = [20]\n",
    "    config.train_only_connected_masks=False\n",
    "    config.eval_only_connected_masks=False\n",
    "    config.evaluate_num_batches = 3\n",
    "    config.deactivate3Devaluation = False\n",
    "    config.dataset_train_path = \"./datasets/synthesis/dataset_eval/imgs\"\n",
    "    config.segm_train_path = \"./datasets/synthesis/dataset_eval/segm\"\n",
    "    config.masks_train_path = \"./datasets/synthesis/dataset_eval/masks\" \n",
    "    config.num_sorted_samples = 1\n",
    "    config.num_dataloader_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ffd20-bc6e-49b8-a330-ce0b03c55287",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(config.eval_loss_timesteps) == config.eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 12:12:18.568276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 12:12:18.818234: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 12:12:18.818287: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 12:12:18.818340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 12:12:18.865285: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 12:12:18.866380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 12:12:23.645181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65f7d2c666644958f2cc206d6a4cf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401254bb5e446c88cb72fe92aef4075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7898cb30054d61af24923f113da79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from custom_modules import DatasetMRI2D, DatasetMRI3D, ScaleDecorator\n",
    "from pathlib import Path\n",
    "from torchvision import transforms \n",
    "\n",
    "#add augmentation\n",
    "transformations = None\n",
    "if config.brightness_augmentation:\n",
    "    transformations = transforms.RandomApply([ScaleDecorator(transforms.ColorJitter(brightness=1))], p=0.5)\n",
    "\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI2D(root_dir_img=Path(config.dataset_train_path), root_dir_segm=Path(config.segm_train_path), only_connected_masks=config.train_only_connected_masks, t1n_target_shape=config.t1n_target_shape, num_sorted_samples=config.num_sorted_samples, transforms=transformations)\n",
    "datasetEvaluation = DatasetMRI2D(root_dir_img=Path(config.dataset_eval_path), root_dir_masks=Path(config.masks_eval_path), root_dir_synthesis=Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks, t1n_target_shape=config.t1n_target_shape, num_sorted_samples=config.num_sorted_samples)\n",
    "dataset3DEvaluation = DatasetMRI3D(root_dir_img=Path(config.dataset_eval_path), root_dir_masks=Path(config.masks_eval_path), root_dir_synthesis=Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks, t1n_target_shape=config.t1n_target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799bdb3-4c83-4ab8-8e46-33d2fe30f49d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1188218d-a16d-4c0a-896f-e47a140bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from custom_modules import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.unet_img_shape,  # the target image resolution\n",
    "    in_channels=config.channels,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"Pseudo3DDownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"Pseudo3DDownBlock2D\",\n",
    "        \"Pseudo3DDownBlock2D\",\n",
    "        \"Pseudo3DDownBlock2D\",\n",
    "        \"Pseudo3DAttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"Pseudo3DDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"Pseudo3DUpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"Pseudo3DAttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"Pseudo3DUpBlock2D\",\n",
    "        \"Pseudo3DUpBlock2D\",\n",
    "        \"Pseudo3DUpBlock2D\",\n",
    "        \"Pseudo3DUpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.model = \"Pseudo3DUNet2DModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a40c63-4ba1-44aa-8c62-7f15ce816225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "config.noise_scheduler = \"DDIMScheduler(num_train_timesteps=1000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f42d0a-9ebb-4437-b180-12b79af3c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "config.lr_scheduler = \"cosine_schedule_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ff284a-4306-4677-829e-98fef153fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from custom_modules import TrainingUnconditional, GuidedRePaintPipeline, Evaluation2DSynthesis, Evaluation3DSynthesis \n",
    "from custom_modules import PipelineFactories\n",
    "\n",
    "config.conditional_data = \"None\"\n",
    "\n",
    "args = {\n",
    "    \"config\": config, \n",
    "    \"model\": model, \n",
    "    \"noise_scheduler\": noise_scheduler, \n",
    "    \"optimizer\": optimizer, \n",
    "    \"lr_scheduler\": lr_scheduler, \n",
    "    \"datasetTrain\": datasetTrain, \n",
    "    \"datasetEvaluation\": datasetEvaluation, \n",
    "    \"dataset3DEvaluation\": dataset3DEvaluation, \n",
    "    \"evaluation2D\": Evaluation2DSynthesis,\n",
    "    \"evaluation3D\": Evaluation3DSynthesis, \n",
    "    \"pipelineFactory\": PipelineFactories.get_guided_repaint_pipeline, \n",
    "    \"multi_sample\": config.num_sorted_samples > 1,\n",
    "    \"deactivate3Devaluation\": config.deactivate3Devaluation,\n",
    "    \"evaluation_pipeline_parameters\": {\n",
    "                \"jump_length\": config.jump_length,\n",
    "                \"jump_n_sample\": config.jump_n_sample,\n",
    "            }} \n",
    "trainingSynthesis = TrainingUnconditional(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac4e0a-2837-4217-a97c-ce940bdc2151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d9c0db95c844b6923bfa82c6c95533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator  None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a05541c94c445f9913c09dc02b51967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator  <torch._C.Generator object at 0x7f6f55f311b0>\n",
      "median lesion intensity:  tensor(-0.1110)\n"
     ]
    }
   ],
   "source": [
    "if config.mode == \"train\":\n",
    "    trainingSynthesis.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7971-c0ac-461d-9fcf-78481e7c9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"eval\":\n",
    "    trainingSynthesis.config.deactivate3Devaluation = False\n",
    "    pipeline = GuidedRePaintPipeline.from_pretrained(config.output_dir) \n",
    "    trainingSynthesis.evaluate(pipeline, deactivate_save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e6ddc-f89d-451d-9a79-4edc40c05230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "if config.mode == \"tuning_parameters\":\n",
    "    trainingSynthesis.config.deactivate3Devaluation = False\n",
    "    pipeline = GuidedRePaintPipeline.from_pretrained(config.output_dir)\n",
    "    original_output_dir = config.output_dir \n",
    "    timesteps = [1, 3, 5]\n",
    "    resample_step = [1, 3, 5]\n",
    "    parameters = product(timesteps, resample_step) \n",
    "    \n",
    "    for timestep, resample_step in parameters:\n",
    "        print(\"Begin evaluation for timestep \", timestep, \" and resample step \", resample_step)\n",
    "        trainingSynthesis.config.intermediate_timestep = timestep\n",
    "        trainingSynthesis.config.jump_length = timestep\n",
    "        trainingSynthesis.config.jump_n_sample = resample_step\n",
    "        trainingSynthesis.config.output_dir = original_output_dir + \"/tuning_parameters/timestep_\" + str(timestep) + \"_resample_\" + str(resample_step)\n",
    "        if trainingSynthesis.accelerator.is_main_process:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "            trainingSynthesis.log_meta_logs()\n",
    "        trainingSynthesis.evaluate(pipeline, deactivate_save_model=True) \n",
    "\n",
    "    # plot lpips score vs  \n",
    "    folder_list = list(Path().glob(original_output_dir + \"/tuning_parameters/timestep_*\"))\n",
    "    lpips = []\n",
    "    labels = [] \n",
    "    for folder in folder_list:\n",
    "        timestep = str(folder).split(\"_\")[-3] \n",
    "        resample_step = str(folder).split(\"_\")[-1]\n",
    "        with open(folder / \"metrics.csv\", 'r') as fp: \n",
    "            _ = fp.readline()\n",
    "            csv_metrics = fp.readline()\n",
    "            reader = csv_metrics.split(',') \n",
    "            for metric in reader:\n",
    "                if metric != \"\":\n",
    "                    name, value = metric.split(':')\n",
    "                    if name == \"lpips\":\n",
    "                        lpips.append(float(value))\n",
    "                        labels.append(timestep + \"_\" + resample_step) \n",
    "    plt.clf()\n",
    "    plt.bar(labels, lpips) \n",
    "    plt.savefig(original_output_dir + '/lpips_parameters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be170a32-fcb6-41b1-b6ea-14b77eb7b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_synthesis_3D_unconditioned.ipynb to script\n",
      "[NbConvertApp] Writing 9601 bytes to lesion_synthesis_3D_unconditioned.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix \n",
    "import os\n",
    "\n",
    "!jupyter nbconvert --to script \"lesion_synthesis_3D_unconditioned.ipynb\"\n",
    "filename=\"lesion_synthesis_3D_unconditioned.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
