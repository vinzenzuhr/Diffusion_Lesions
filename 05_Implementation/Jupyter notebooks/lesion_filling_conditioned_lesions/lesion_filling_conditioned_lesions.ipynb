{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6550fa47-7836-455a-9f39-483b9977ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../custom_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77b51cd-dfaa-44b4-b1f6-a36e07016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1 \n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "    num_epochs = 90 #600 # one epoch needs ~12min (x2 GPU)\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 100 #500\n",
    "    evaluate_epochs = 20\n",
    "    evaluate_num_batches = 20 # one batch needs ~15s. \n",
    "    evaluate_save_img_epochs = 20\n",
    "    evaluate_3D_epochs = 1000  # one 3D evaluation needs ~20min\n",
    "    save_model_epochs = 60\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-cond-lesions\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./dataset_train/imgs\"\n",
    "    segm_train_path = \"./dataset_train/segm\"\n",
    "    masks_train_path = \"./dataset_train/masks\"\n",
    "    dataset_eval_path = \"./dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./dataset_eval/segm\"\n",
    "    masks_eval_path = \"./dataset_eval/masks\"\n",
    "    num_gpu=2\n",
    "    train_only_connected_masks=True\n",
    "    eval_only_connected_masks=False\n",
    "    num_inference_steps=50\n",
    "    debug=True\n",
    "    #uniform_dataset_path = \"./uniform_dataset\"\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37af310-fb03-4445-a931-bc7e4f359ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps=5\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1\n",
    "    config.num_gpu=1\n",
    "    config.train_only_connected_masks=False\n",
    "    config.eval_only_connected_masks=False\n",
    "    config.evaluate_num_batches=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e2ffca-7e8c-43d4-8e13-ba1900e38eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jovyan/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0745d4bc248045a989ddc38c7541ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0abbe288d514b318bc1866ef74d2f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f80f387b58e485c8c3d3b35cdee06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2047\n",
      "\tImage shape: torch.Size([1, 256, 256])\n",
      "Training Data: ['gt_image', 'segm', 'mask', 'max_v', 'idx', 'name']\n"
     ]
    }
   ],
   "source": [
    "from DatasetMRI2D import DatasetMRI2D\n",
    "from DatasetMRI3D import DatasetMRI3D\n",
    "from pathlib import Path\n",
    "\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI2D(Path(config.dataset_train_path), Path(config.segm_train_path), Path(config.masks_train_path), only_connected_masks=config.train_only_connected_masks)\n",
    "datasetEvaluation = DatasetMRI2D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "dataset3DEvaluation = DatasetMRI3D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b10865-fac0-4151-a227-d426c2fd20da",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a6fe06-53ba-4056-bffb-c1c4acecacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7f52873b4f40> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f52925d8400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get 6 random sample\n",
    "random_indices = np.random.randint(0, len(datasetTrain) - 1, size=(6)) \n",
    "\n",
    "# Plot: t1n images\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f7adb-e647-4260-a1cf-20bd8a3f2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: masks\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow(datasetTrain[idx][\"mask\"].squeeze())\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099f07-f1ff-46c1-bc83-2e448b80e778",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41941ef-16c4-4b3c-8e6f-712fee5b538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "import random\n",
    "\n",
    "#reset seed every time a worker loads a dataset \n",
    "def reset_seed(worker_id=0): \n",
    "    np.random.seed(config.seed) \n",
    "    torch.manual_seed(config.seed)\n",
    "    torch.cuda.manual_seed_all(config.seed)\n",
    "    random.seed(config.seed)\n",
    "    return\n",
    "\n",
    "# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\n",
    "def get_dataloader(dataset, batch_size, num_workers=4, random_sampler=False): \n",
    "    sampler = RandomSampler(dataset, generator=(None if random_sampler else torch.cuda.manual_seed_all(config.seed)))\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, sampler=sampler, worker_init_fn=(None if random_sampler else reset_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509adcf-a0b1-4652-a5ca-b398b008dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3, # the number of input channels: 1 for img, 1 for img_voided, 1 for mask\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "#tb_summary.add_text(\"model\", \"UNet2DModel\", 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f67f7-b932-4f73-bdb3-96751ef6c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "sample_image = datasetTrain[0]['gt_image'].unsqueeze(0)\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "#tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c3aa-3a69-4dfa-bbb2-d3c8086fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "\n",
    "#tb_summary.add_text(\"lr_scheduler\", \"cosine_schedule_with_warmup\", 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4dfbe-cc66-4b78-8f36-6a5dae10abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from diffusers import DiffusionPipeline, DDIMScheduler, ImagePipelineOutput\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "class DDIMInpaintPipeline(DiffusionPipeline):\n",
    "    \n",
    "        Pipeline for image inpainting.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet2DModel`]):\n",
    "            A `UNet2DModel` to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \n",
    "\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet, scheduler):\n",
    "        super().__init__()\n",
    "\n",
    "        # make sure scheduler can always be converted to DDIM\n",
    "        scheduler = DDIMScheduler.from_config(scheduler.config)\n",
    "\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        voided_imgs: torch.tensor,\n",
    "        masks: torch.tensor,\n",
    "        batch_size: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        eta: float = 0.0,\n",
    "        num_inference_steps: int = 50,\n",
    "        use_clipped_model_output: Optional[bool] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        r\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            voided_imgs ('torch.tensor'): \n",
    "                Images (1 channel) which should be inpainted.\n",
    "            masks ('torch.tensor'): \n",
    "                Binary masks which includes the area to inpaint.\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
    "                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers. A value of `0` corresponds to\n",
    "                DDIM and `1` corresponds to DDPM.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            use_clipped_model_output (`bool`, *optional*, defaults to `None`):\n",
    "                If `True` or `False`, see documentation for [`DDIMScheduler.step`]. If `None`, nothing is passed\n",
    "                downstream to the scheduler (use `None` for schedulers which don't support this argument).\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \n",
    "\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels-2, # Minus the two channels for the mask and the img to be inpainted\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (batch_size, self.unet.config.in_channels-2, *self.unet.config.sample_size) # Minus the two channels for the mask and the img to be inpainted\n",
    "\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        image = randn_tensor(image_shape, generator=generator, device=self._execution_device, dtype=self.unet.dtype)\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        #Input to unet model is concatenation of images, voided images and masks\n",
    "        input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(input, t).sample\n",
    "\n",
    "            # 2. predict previous mean of image x_t-1 and add variance depending on eta\n",
    "            # eta corresponds to η in paper and should be between [0, 1]\n",
    "            # do x_t -> x_t-1\n",
    "            image = self.scheduler.step(\n",
    "                model_output, t, image, eta=eta, use_clipped_model_output=use_clipped_model_output, generator=generator\n",
    "            ).prev_sample\n",
    "\n",
    "            #3. Concatenate image with voided images and masks\n",
    "            input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return ImagePipelineOutput(images=image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b857ce-da16-4622-9c20-3d2572f208a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def generate_masks(n, device, generator=None):\n",
    "    #create circular mask with random center around the center point of the pictures and a radius between 3 and 50 pixels\n",
    "    center=torch.normal(mean=config.image_size/2, std=30, size=(n,2), generator=generator, device=device) # 30 is chosen by inspection\n",
    "    low=3   \n",
    "    high=50\n",
    "    radius=torch.rand(n, device=device, generator=generator)*(high-low)+low # get radius between 3 and 50 from uniform distribution \n",
    "\n",
    "    #Test case\n",
    "    #center=torch.tensor([[0,255],[0,255]]) \n",
    "    #radius=torch.tensor([2,2])\n",
    "    \n",
    "    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "    dist_from_center = dist_from_center.permute(2,0,1) \n",
    "\n",
    "    #Test case\n",
    "    #print(dist_from_center[0,0,0]) #=255\n",
    "    #print(dist_from_center[0,0,255]) #=360.624\n",
    "    #print(dist_from_center[0,255,0]) #=0\n",
    "    #print(dist_from_center[0,255,255]) #=255\n",
    "    #print(dist_from_center[0,127,127]) #=180.313 \n",
    "    \n",
    "    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \n",
    "    masks = masks[:,None,:,:].int() \n",
    "    return masks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d4bff-ad45-4252-96bd-e6e01b1c5d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c47570-9446-4bf1-bf5b-c1bff62a11a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9b9dd-4683-4520-8ba1-884593174c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab2fbe-2011-4117-9e3f-d072c74ff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from torcheval.metrics import PeakSignalNoiseRatio\n",
    "from skimage.metrics import structural_similarity, mean_squared_error\n",
    "\n",
    "def _calc_metrics(image1, image2):\n",
    "    # PSNR metric\n",
    "    metric = PeakSignalNoiseRatio(device=image1.device)\n",
    "    metric.update(image1, image2)\n",
    "    PSNR = metric.compute().item()\n",
    "\n",
    "    # SSIM metric (torch_eval is outdated)\n",
    "    batch_size = image1.shape[0]\n",
    "    mssim_sum=0\n",
    "    for idx in range(batch_size):\n",
    "        mssim = structural_similarity(\n",
    "            image1[idx].detach().cpu().numpy(),\n",
    "            image2[idx].detach().cpu().numpy(),\n",
    "            channel_axis=0,\n",
    "            data_range=2\n",
    "        )\n",
    "        mssim_sum += mssim\n",
    "    SSIM = mssim_sum / batch_size\n",
    "\n",
    "    # MSE metric\n",
    "    mse_sum=0\n",
    "    for idx in range(batch_size):\n",
    "        mse = mean_squared_error(\n",
    "            image1[idx].detach().cpu().numpy(),\n",
    "            image2[idx].detach().cpu().numpy(), \n",
    "        )\n",
    "        mse_sum += mse\n",
    "    MSE = mse_sum / batch_size\n",
    "\n",
    "    return PSNR, SSIM, MSE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cf83f-952c-4ce1-aa97-da35f5b13acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def _log_metrics(tb_summary, global_step, PSNR_mean, SSIM_mean, MSE_mean):\n",
    "    tb_summary.add_scalar(\"PSNR_global\", PSNR_mean, global_step) \n",
    "    tb_summary.add_scalar(\"SSIM_global\", SSIM_mean, global_step)\n",
    "    tb_summary.add_scalar(\"MSE_global\", MSE_mean, global_step)\n",
    "    print(\"SSIM_global: \", SSIM_mean)\n",
    "    print(\"PSNR_global: \", PSNR_mean)\n",
    "    print(\"MSE_global: \", MSE_mean)\n",
    "    print(\"global_step: \", global_step)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0cdc9-1a7e-47f6-bd10-6f28fae1fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import PIL \n",
    "\n",
    "def _save_image(images: list[list[PIL.Image]], titles: list[str], path: Path, epoch: int):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for image_list, title in zip(images, titles): \n",
    "        image_grid = make_image_grid(image_list, rows=int(len(image_list)**0.5), cols=int(len(image_list)**0.5))\n",
    "        image_grid.save(f\"{path}/{title}_{epoch:04d}.png\")\n",
    "    print(\"image saved\")   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5efb3d-3fc5-456b-afc8-40220ff5a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#setup evaluation\n",
    "from diffusers.utils import make_image_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline, eval_dataloader, global_step, tb_summary):\n",
    "    #initialize metrics\n",
    "    PSNR_mean = 0\n",
    "    SSIM_mean = 0  \n",
    "    MSE_mean = 0\n",
    "\n",
    "    reset_seed(0)\n",
    "    for n_iter, batch in enumerate(eval_dataloader): \n",
    "        if n_iter >= config.evaluate_num_batches:\n",
    "            break\n",
    "            \n",
    "        # get batch\n",
    "        clean_images = batch[\"gt_image\"]\n",
    "        masks = batch[\"mask\"]\n",
    "        voided_images = clean_images*masks \n",
    "\n",
    "        # run them through pipeline\n",
    "        inpainted_images = pipeline(\n",
    "            voided_images,\n",
    "            masks,\n",
    "            batch_size=masks.shape[0],\n",
    "            generator=torch.cuda.manual_seed_all(config.seed),\n",
    "            output_type=np.array,\n",
    "            num_inference_steps = config.num_inference_steps\n",
    "        ).images\n",
    "        inpainted_images = torch.from_numpy(inpainted_images).to(clean_images.device)\n",
    "        \n",
    "        # transform from B x H x W x C to B x C x H x W \n",
    "        inpainted_images = torch.permute(inpainted_images, (0, 3, 1, 2))\n",
    "\n",
    "        PSNR, SSIM, MSE = _calc_metrics(clean_images, inpainted_images)\n",
    "        PSNR_mean += PSNR\n",
    "        SSIM_mean += SSIM \n",
    "        MSE_mean += MSE\n",
    "        \n",
    "    # calculcate mean of metrics\n",
    "    PSNR_mean /= len(eval_dataloader)\n",
    "    SSIM_mean /= len(eval_dataloader)\n",
    "    MSE_mean /= len(eval_dataloader)\n",
    "\n",
    "    _log_metrics(tb_summary, global_step, PSNR_mean, SSIM_mean, MSE_mean)\n",
    "\n",
    "    # save last batch as sample images\n",
    "    if (epoch) % config.evaluate_save_img_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "        # change range from [-1,1] to [0,1]\n",
    "        voided_images = (voided_images+1)/2\n",
    "        clean_images = (clean_images+1)/2\n",
    "        # change binary image from 0,1 to 0,255\n",
    "        masks = masks*255\n",
    "\n",
    "        # save images\n",
    "        list = [inpainted_images, voided_images, clean_images, masks]\n",
    "        title_list = [\"inpainted_images\", \"voided_images\", \"clean_images\", \"masks\"] \n",
    "        image_list = [[to_pil_image(x, mode=\"L\") for x in images] for images in list]\n",
    "        _save_image(image_list, title_list, os.path.join(config.output_dir, \"samples_2D\"), epoch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf8419-a8d9-4b8a-81e4-a9fb0f1b1cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80c12a-336a-4bc4-95c2-a82d4c78d6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe9577-b74c-4390-8c3a-c6ad38e5af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "\n",
    "def dim3evaluate(config, epoch, pipeline, dim3eval_dataloader, global_step, tb_summary):\n",
    "    #unused: global_step, tb_summary\n",
    "    print(\"Start 3D evaluation\")\n",
    "    #batch = next(iter(dim3eval_dataloader)) #TODO: Anpassen, falls grösseres evaluation set\n",
    "\n",
    "    # go through every batch\n",
    "    for batch in dim3eval_dataloader:\n",
    "        # go through sample in batch\n",
    "        for sample_idx in torch.arange(batch[\"gt_image\"].shape[0]):\n",
    "            clean_images = batch[\"gt_image\"][sample_idx] #torch.Size([1, 256, 256, 256])\n",
    "            masks = batch[\"mask\"][sample_idx]  #torch.Size([1, 256, 256, 256])\n",
    "            max_v = batch[\"max_v\"][sample_idx]\n",
    "            idx = batch[\"idx\"][sample_idx]\n",
    "        \n",
    "            voided_images = clean_images*masks\n",
    "        \n",
    "            #get slices which have to be inpainted\n",
    "            slice_indices = []\n",
    "            for slice_idx in torch.arange(voided_images.shape[2]):\n",
    "                if (1-masks[:, :, slice_idx, :]).any():\n",
    "                    slice_indices.append(slice_idx.unsqueeze(0)) \n",
    "            slice_indices = torch.cat(slice_indices, 0)\n",
    "        \n",
    "            #create chunks of slices which have to be inpainted\n",
    "            stacked_void_mask = torch.stack((voided_images[:, :, slice_indices, :], masks[:, :, slice_indices, :]), dim=0)\n",
    "            stacked_void_mask = stacked_void_mask.permute(0, 3, 1, 2, 4) \n",
    "            chunks = torch.chunk(stacked_void_mask, math.ceil(stacked_void_mask.shape[1]/config.eval_batch_size), dim=1)\n",
    "            \n",
    "            #inpaint all slices\n",
    "            inpainted_images = [] \n",
    "            for chunk in chunks:\n",
    "                chunk_voided_images = chunk[0]\n",
    "                chunk_masks = chunk[1]\n",
    "\n",
    "                size = chunk_masks.shape[0]\n",
    "                \n",
    "                images = pipeline(\n",
    "                    chunk_voided_images,\n",
    "                    chunk_masks,\n",
    "                    batch_size=size,\n",
    "                    generator=torch.cuda.manual_seed_all(config.seed),\n",
    "                    output_type=np.array,\n",
    "                    num_inference_steps = config.num_inference_steps\n",
    "                ).images\n",
    "                inpainted_images.append(torch.from_numpy(images))\n",
    "            inpainted_images = torch.cat(inpainted_images, dim=0)\n",
    "            inpainted_images = inpainted_images.permute(3, 1, 0, 2)\n",
    "            inpainted_images = inpainted_images.to(clean_images.device())\n",
    "        \n",
    "            #overwrite the original 3D image with the inpainted 2D slices\n",
    "            clean_images[:, :, slice_indices, :] = inpainted_images\n",
    "        \n",
    "            #postprocess and save image as nifti file\n",
    "            clean_images = datasetEvaluation.postprocess(clean_images, max_v)\n",
    "        \n",
    "            test_dir = os.path.join(config.output_dir, \"samples_3D\")\n",
    "            os.makedirs(test_dir, exist_ok=True)\n",
    "            \n",
    "            datasetEvaluation.save(clean_images, f\"{test_dir}/{idx}_{epoch:04d}.nii.gz\", **datasetEvaluation.get_metadata(int(idx)))\n",
    "        \n",
    "    print(\"Finish 3D evaluation\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d20651-5544-4864-822a-b30dcb5d1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_logs(tb_summary):\n",
    "    #log at tensorboard\n",
    "    tb_summary.add_scalar(\"image_size\", config.image_size, 0)\n",
    "    tb_summary.add_scalar(\"train_batch_size\", config.train_batch_size, 0)\n",
    "    tb_summary.add_scalar(\"eval_batch_size\", config.eval_batch_size, 0)\n",
    "    tb_summary.add_scalar(\"num_epochs\", config.num_epochs, 0)\n",
    "    tb_summary.add_scalar(\"learning_rate\", config.learning_rate, 0)\n",
    "    tb_summary.add_scalar(\"lr_warmup_steps\", config.lr_warmup_steps, 0)\n",
    "    tb_summary.add_scalar(\"evaluate_epochs\", config.evaluate_epochs, 0)\n",
    "    tb_summary.add_scalar(\"evaluate_save_img_epochs\", config.evaluate_save_img_epochs, 0)\n",
    "    tb_summary.add_scalar(\"evaluate_3D_epochs\", config.evaluate_3D_epochs, 0) \n",
    "    tb_summary.add_text(\"mixed_precision\", config.mixed_precision, 0) \n",
    "    tb_summary.add_scalar(\"train_only_connected_masks\", config.train_only_connected_masks, 0)\n",
    "    tb_summary.add_scalar(\"eval_only_connected_masks\", config.eval_only_connected_masks, 0) \n",
    "    tb_summary.add_scalar(\"debug\", config.debug, 0) \n",
    "    tb_summary.add_text(\"conditional_data\", \"Lesions\", 0) \n",
    "    tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) \n",
    "    tb_summary.add_text(\"lr_scheduler\", \"cosine_schedule_with_warmup\", 0)\n",
    "    tb_summary.add_text(\"model\", \"UNet2DModel\", 0) \n",
    "    tb_summary.add_text(\"inference_pipeline\", \"DDIMPipeline\", 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6078de-93ba-4c23-8f45-6b2e737cd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import custom modules\n",
    "from DDIMInpaintPipeline import DDIMInpaintPipeline\n",
    "from Evaluation2D import Evaluation2D\n",
    "from Evaluation3D import Evaluation3D\n",
    "\n",
    "#import other modules\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, lr_scheduler):\n",
    "    # setup training environment\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        project_dir=os.path.join(config.output_dir, \"tensorboard\"),\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        #setup tensorboard\n",
    "        tb_summary = SummaryWriter(config.output_dir, purge_step=0)\n",
    "        meta_logs(tb_summary)\n",
    "        \n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True) \n",
    "        segmentation_dir = os.path.join(config.output_dir, \"segmentations_3D\")\n",
    "        os.makedirs(segmentation_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    train_dataloader = get_dataloader(dataset = datasetTrain, batch_size = config.train_batch_size, random_sampler=True)\n",
    "    d2_eval_dataloader = get_dataloader(dataset = datasetEvaluation, batch_size = config.eval_batch_size, random_sampler=False)\n",
    "    d3_eval_dataloader = get_dataloader(dataset = dataset3DEvaluation, batch_size = 1, random_sampler=False) \n",
    "\n",
    "    model, optimizer, train_dataloader, d2_eval_dataloader, d3_eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, d2_eval_dataloader, d3_eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)  \n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #Test eval functions at the beginning of training\n",
    "    #if accelerator.is_main_process:\n",
    "    #    epoch=0\n",
    "    #    model.eval()\n",
    "    #    pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)  \n",
    "    #    evaluate(config, epoch, pipeline, d2_eval_dataloader, global_step, tb_summary)\n",
    "    #    dim3evaluate(config, epoch, pipeline, d3_eval_dataloader, global_step, tb_summary)\n",
    "    \n",
    "    \n",
    "    # Train model\n",
    "    model.train()\n",
    "    for epoch in range(config.num_epochs): \n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process) \n",
    "        progress_bar.set_description(f\"Epoch {epoch}\") \n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            \n",
    "            clean_images = batch[\"gt_image\"]\n",
    "            masks = batch[\"mask\"]\n",
    "            print(batch[\"name\"])\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            \n",
    "            #create voided img\n",
    "            voided_images = clean_images*masks\n",
    "\n",
    "            # Add noise to the voided images according to the noise magnitude at each timestep (forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) \n",
    "\n",
    "            # concatenate noisy_images, voided_images and mask\n",
    "            input=torch.cat((noisy_images, voided_images, masks), dim=1)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(input, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "                #log gradient norm \n",
    "                parameters = [p for p in model.parameters() if p.grad is not None and p.requires_grad]\n",
    "                if len(parameters) == 0:\n",
    "                    total_norm = 0.0\n",
    "                else: \n",
    "                    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()).cpu() for p in parameters]), 2.0).item()\n",
    "\n",
    "                #do learning step\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # save logs\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {\"loss\": loss.cpu().detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"total_norm\": total_norm, \"step\": global_step}\n",
    "                tb_summary.add_scalar(\"loss\", logs[\"loss\"], global_step)\n",
    "                tb_summary.add_scalar(\"lr\", logs[\"lr\"], global_step) \n",
    "                tb_summary.add_scalar(\"total_norm\", logs[\"total_norm\"], global_step) \n",
    "            \n",
    "                progress_bar.set_postfix(**logs)\n",
    "                #accelerator.log(logs, step=global_step)\n",
    "            global_step += 1 \n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        # After a certain number of epochs it samples some images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            model.eval()\n",
    "            pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            pipeline = accelerator.prepare(pipeline)\n",
    "    \n",
    "            if (epoch) % config.evaluate_epochs == 0 or epoch == config.num_epochs - 1: \n",
    "                eval = Evaluation2D(config, pipeline, d2_eval_dataloader, tb_summary)\n",
    "                eval.evaluate(epoch, global_step)\n",
    "    \n",
    "            if (epoch) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1: \n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "            if (epoch) % config.evaluate_3D_epochs == 0 or epoch == config.num_epochs - 1: \n",
    "                eval = Evaluation3D(config, pipeline, d3_eval_dataloader, tb_summary)  \n",
    "                eval.evaluate(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f42e5-88c7-4298-8ad7-2b7b4555a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import notebook_launcher\n",
    "\n",
    "# If run from a jupyter notebook then uncomment the two lines and remove last line\n",
    "args = {\"config\": config, \"model\": model, \"noise_scheduler\": noise_scheduler, \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "#notebook_launcher(train_loop, args, num_processes=config.num_gpu)\n",
    "#notebook_launcher(train_loop, args, num_processes=1)\n",
    "\n",
    "train_loop(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57c0c6-43be-4c06-90bc-3e50b56a8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fb612-dede-46d9-abd1-ed1cc23b01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create python script for ubelix\n",
    "!jupyter nbconvert --to script \"lesion_filling_conditioned_lesions.ipynb\"\n",
    "filename=\"lesion_filling_conditioned_lesions.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-18: \n",
    "            fp.write(line)\n",
    "# Deactivate Debug!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
