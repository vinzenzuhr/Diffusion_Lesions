{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77b51cd-dfaa-44b4-b1f6-a36e07016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1 \n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "    num_epochs = 80 #600\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 100 #500\n",
    "    evaluate_epochs = 2 #30\n",
    "    save_model_epochs = 60 # 300\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-cond-lesions\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./dataset_train/imgs\"\n",
    "    segm_train_path = \"./dataset_train/segm\"\n",
    "    dataset_eval_path = \"./dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./dataset_eval/segm\"\n",
    "    num_gpu=2\n",
    "    #uniform_dataset_path = \"./uniform_dataset\"\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e2ffca-7e8c-43d4-8e13-ba1900e38eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 08:12:33.088873: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 08:12:34.180725: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-21 08:12:34.180776: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-21 08:12:34.184900: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-21 08:12:34.808462: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 08:12:34.825823: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 08:12:40.571262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0285a5c6-3169-4eeb-9dc3-afa90494b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "class DatasetMRI(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Training purposes. \n",
    "    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\n",
    "    \n",
    "    Contains ground truth t1n images (gt) \n",
    "    Args:\n",
    "        root_dir_img: Path to img files\n",
    "        root_dir_segm: Path to segmentation maps\n",
    "        pad_shape: Shape the images will be transformed to\n",
    "\n",
    "    Raises:\n",
    "        UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "    Returns: \n",
    "        __getitem__: Returns a dictoinary containing:\n",
    "            \"gt_image\": Padded and cropped version of t1n 2D slice\n",
    "            \"segm\": Segmentation of 2D slice\n",
    "            \"t1n_path\": Path to the unpadded t1n file for this sample\n",
    "            \"max_v\": Maximal value of t1 image (used for normalization)\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256), only_white_matter=False):\n",
    "        #Initialize variables\n",
    "        self.root_dir_img = root_dir_img\n",
    "        self.root_dir_segm = root_dir_segm\n",
    "        self.pad_shape = pad_shape \n",
    "        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\n",
    "        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\n",
    "        #define offsets between first and last segmented slices and the slices to be used for training\n",
    "        bottom_offset=60\n",
    "        top_offset=20\n",
    "\n",
    "        #go through all 3D imgs\n",
    "        idx=0\n",
    "        self.idx_to_2D_slice = dict()\n",
    "        for j, path in enumerate(self.list_paths_segm):\n",
    "            t1n_segm = nib.load(path)\n",
    "            t1n_3d = t1n_segm.get_fdata()\n",
    "\n",
    "            #transform segmentation\n",
    "            t1n_3d = np.transpose(t1n_3d)\n",
    "            t1n_3d = np.flip(t1n_3d, axis=1)\n",
    "\n",
    "            #get first slice with white matter or segmented content plus offset\n",
    "            i=0\n",
    "            if(only_white_matter):\n",
    "                #while there is no white matter go to the next slide\n",
    "                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\n",
    "                    i += 1\n",
    "                bottom = i\n",
    "            else:\n",
    "                #while there is no segmented tissue go to the next slide\n",
    "                while(not t1n_3d[:,i,:].any()):\n",
    "                    i += 1\n",
    "                bottom = i + bottom_offset \n",
    "\n",
    "            #get last slice with white matter or segmented content minus offset\n",
    "            i=t1n_3d.shape[1]-1\n",
    "            if(only_white_matter):\n",
    "                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\n",
    "                    i -= 1\n",
    "                top = i\n",
    "            else:\n",
    "                while(not t1n_3d[:,i,:].any()):\n",
    "                    i -= 1\n",
    "                top = i - top_offset \n",
    "            \n",
    "            #Add all slices between desired top and bottom slice to dataset\n",
    "            for i in np.arange(top-bottom):\n",
    "                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j], self.list_paths_segm[j],bottom+i, [])\n",
    "                idx+=1   \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.idx_to_2D_slice.keys()) \n",
    "\n",
    "    def preprocess(self, t1n: np.ndarray):\n",
    "        \"\"\"\n",
    "        Transforms the images to a more unified format.\n",
    "        Normalizes to -1,1. Pad and crop to bounding box.\n",
    "        \n",
    "        Args:\n",
    "            t1n (np.ndarray): t1n from t1n file (ground truth).\n",
    "\n",
    "        Raises:\n",
    "            UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "        Returns:\n",
    "            t1n: The padded and cropped version of t1n.\n",
    "            t1n_max_v: Maximal value of t1n image (used for normalization).\n",
    "        \"\"\"\n",
    "\n",
    "        #Size assertions\n",
    "        reference_shape = (256,256,160)\n",
    "        if t1n.shape != reference_shape:\n",
    "            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\n",
    "\n",
    "        #Normalize the image to [0,1]\n",
    "        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\n",
    "        t1n_max_v = np.max(t1n)\n",
    "        t1n /= t1n_max_v\n",
    "\n",
    "        #pad to bounding box\n",
    "        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\n",
    "        t1n = torch.Tensor(t1n)\n",
    "        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\n",
    "        d_max, w_max, h_max = size\n",
    "        d_pad = max((d_max - d) / 2, 0)\n",
    "        w_pad = max((w_max - w) / 2, 0)\n",
    "        h_pad = max((h_max - h) / 2, 0)\n",
    "        padding = (\n",
    "            int(floor(h_pad)),\n",
    "            int(ceil(h_pad)),\n",
    "            int(floor(w_pad)),\n",
    "            int(ceil(w_pad)),\n",
    "            int(floor(d_pad)),\n",
    "            int(ceil(d_pad)),\n",
    "        )\n",
    "        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \n",
    "\n",
    "        #map images from [0,1] to [-1,1]\n",
    "        t1n = (t1n*2) - 1\n",
    "\n",
    "        return t1n, t1n_max_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1n_path = self.idx_to_2D_slice[idx][0]\n",
    "        segm_path = self.idx_to_2D_slice[idx][1]\n",
    "        slice_idx = self.idx_to_2D_slice[idx][2]\n",
    "        masks = self.idx_to_2D_slice[idx][3]\n",
    "        \n",
    "        t1n_img = nib.load(t1n_path)\n",
    "        t1n = t1n_img.get_fdata()\n",
    "        t1n_segm = nib.load(segm_path)\n",
    "        t1n_segm_data = t1n_segm.get_fdata()\n",
    "        \n",
    "        t1n_segm_data = np.transpose(t1n_segm_data)\n",
    "        t1n_segm_data = np.flip(t1n_segm_data, axis=1)\n",
    "        \n",
    "        # preprocess data\n",
    "        t1n, t1n_max_v = self.preprocess(t1n)\n",
    "        \n",
    "        # get 2D slice from 3D\n",
    "        t1n_slice = t1n[:,slice_idx,:] \n",
    "        t1n_segm_slice = t1n_segm_data[:,slice_idx,:]\n",
    "        \n",
    "        # Output data\n",
    "        sample_dict = {\n",
    "            \"gt_image\": t1n_slice.unsqueeze(0),\n",
    "            \"segm\": t1n_segm_slice,\n",
    "            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\n",
    "            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization\n",
    "            \"masks\": masks, #generated masks\n",
    "        }\n",
    "        return sample_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a3463-9d25-4106-b947-50800e5086cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 54\n",
      "\tImage shape: torch.Size([1, 256, 256])\n",
      "Training Data: ['gt_image', 'segm', 't1n_path', 'max_v', 'masks']\n"
     ]
    }
   ],
   "source": [
    "#create dataset\n",
    "datasetTrain = DatasetMRI(Path(config.dataset_train_path), Path(config.segm_train_path), pad_shape=(256, 256, 256), only_white_matter=False)\n",
    "datasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256))\n",
    "\n",
    "print(f\"Dataset size: {len(datasetTrain)}\")\n",
    "print(f\"\\tImage shape: {datasetTrain[0]['gt_image'].shape}\")\n",
    "print(f\"Training Data: {list(datasetTrain[0].keys())}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b10865-fac0-4151-a227-d426c2fd20da",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6286c-155d-4d6c-923d-bef8ce0b180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axis = plt.subplots(1,2,figsize=(16,4))\n",
    "idx=60\n",
    "axis[0].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "axis[1].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "fig.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c57471-c0bf-4bca-9207-4dc6194bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 6 random sample\n",
    "random_indices = np.random.randint(0, len(datasetTrain) - 1, size=(6)) \n",
    "\n",
    "# Plot: t1n segmentations\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6fe06-53ba-4056-bffb-c1c4acecacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: t1n images\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba21352-9edc-4351-a359-de83bbed114d",
   "metadata": {},
   "source": [
    "### Playground for random circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18221bb-ac64-4876-9a87-24a8c0ff7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normal distributions of center points\n",
    "centers=[]\n",
    "for _ in np.arange(100):\n",
    "    centers.append(torch.normal(torch.tensor([127.,127.]),torch.tensor(30.)))\n",
    "\n",
    "plt.imshow((datasetTrain[70][\"gt_image\"].squeeze()+1)/2) \n",
    "for center in centers:\n",
    "    plt.scatter(center[0], center[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6508289-04f3-46c8-bc05-9629d7799e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=torch.zeros((10,256,256)).shape\n",
    "\n",
    "#create circular mask with random center around the center point of the pictures and a radius between 3 and 40 pixels\n",
    "center=np.random.normal([127,127],30, size=(example[0],2))\n",
    "radius=np.random.uniform(low=3,high=40, size=(example[0]))\n",
    "\n",
    "Y, X = np.ogrid[:256, :256] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "dist_from_center = np.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "\n",
    "mask = dist_from_center <= radius # creates mask for pixels which are inside the radius\n",
    "mask = 1-mask\n",
    "\n",
    "plt.imshow(((datasetTrain[70][\"gt_image\"].squeeze()+1)/2)*mask[:,:,4]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa2c12-25b0-41dd-a528-035b7c63f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ellips playground\n",
    "\"\"\"\n",
    "from math import pi\n",
    "\n",
    "mean=torch.normal(torch.tensor([127.,127.]),torch.tensor(30.)) # mean is sampled with normal distribution around the midpoint\n",
    "radius=np.random.uniform(low=3,high=50,size=(2)) # shape of ellipse is sampled with uniform distribution between 3 and 15\n",
    "\n",
    "u=mean[0]     #x-position of the center\n",
    "v=mean[1]    #y-position of the center\n",
    "a=radius[0]     #radius on the x-axis\n",
    "b=radius[1]    #radius on the y-axis\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "t = np.linspace(0, 2*pi, 100)\n",
    "plt.plot( u+a*np.cos(t) , v+b*np.sin(t),color=\"red\") \n",
    "plt.imshow((datasetTrain[500][\"gt_image\"].squeeze()+1)/2)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099f07-f1ff-46c1-bc83-2e448b80e778",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41941ef-16c4-4b3c-8e6f-712fee5b538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7509adcf-a0b1-4652-a5ca-b398b008dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3, # the number of input channels: 1 for img, 1 for img_voided, 1 for mask\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "#tb_summary.add_text(\"model\", \"UNet2DModel\", 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05f67f7-b932-4f73-bdb3-96751ef6c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "sample_image = datasetTrain[0]['gt_image'].unsqueeze(0)\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "\n",
    "#tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5171c3aa-3a69-4dfa-bbb2-d3c8086fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "\n",
    "#tb_summary.add_text(\"lr_scheduler\", \"cosine_schedule_with_warmup\", 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f4dfbe-cc66-4b78-8f36-6a5dae10abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from diffusers import DiffusionPipeline, DDIMScheduler, ImagePipelineOutput\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DDIMInpaintPipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for image inpainting.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet2DModel`]):\n",
    "            A `UNet2DModel` to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \"\"\"\n",
    "\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet, scheduler):\n",
    "        super().__init__()\n",
    "\n",
    "        # make sure scheduler can always be converted to DDIM\n",
    "        scheduler = DDIMScheduler.from_config(scheduler.config)\n",
    "\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        voided_imgs: torch.tensor,\n",
    "        masks: torch.tensor,\n",
    "        batch_size: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        eta: float = 0.0,\n",
    "        num_inference_steps: int = 50,\n",
    "        use_clipped_model_output: Optional[bool] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            voided_imgs ('torch.tensor'): \n",
    "                Images (1 channel) which should be inpainted.\n",
    "            masks ('torch.tensor'): \n",
    "                Binary masks which includes the area to inpaint.\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
    "                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers. A value of `0` corresponds to\n",
    "                DDIM and `1` corresponds to DDPM.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            use_clipped_model_output (`bool`, *optional*, defaults to `None`):\n",
    "                If `True` or `False`, see documentation for [`DDIMScheduler.step`]. If `None`, nothing is passed\n",
    "                downstream to the scheduler (use `None` for schedulers which don't support this argument).\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels-2, # Minus the two channels for the mask and the img to be inpainted\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (batch_size, self.unet.config.in_channels-2, *self.unet.config.sample_size) # Minus the two channels for the mask and the img to be inpainted\n",
    "\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        image = randn_tensor(image_shape, generator=generator, device=self._execution_device, dtype=self.unet.dtype)\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        #Input to unet model is concatenation of images, voided images and masks\n",
    "        input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(input, t).sample\n",
    "\n",
    "            # 2. predict previous mean of image x_t-1 and add variance depending on eta\n",
    "            # eta corresponds to η in paper and should be between [0, 1]\n",
    "            # do x_t -> x_t-1\n",
    "            image = self.scheduler.step(\n",
    "                model_output, t, image, eta=eta, use_clipped_model_output=use_clipped_model_output, generator=generator\n",
    "            ).prev_sample\n",
    "\n",
    "            #3. Concatenate image with voided images and masks\n",
    "            input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return ImagePipelineOutput(images=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b857ce-da16-4622-9c20-3d2572f208a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(n, device, generator=None):\n",
    "    #create circular mask with random center around the center point of the pictures and a radius between 3 and 50 pixels\n",
    "    center=torch.normal(mean=config.image_size/2, std=30, size=(n,2), generator=generator, device=device) # 30 is chosen by inspection\n",
    "    low=3   \n",
    "    high=50\n",
    "    radius=torch.rand(n, device=device, generator=generator)*(high-low)+low # get radius between 3 and 50 from uniform distribution \n",
    "\n",
    "    #Test case\n",
    "    #center=torch.tensor([[0,255],[0,255]]) \n",
    "    #radius=torch.tensor([2,2])\n",
    "    \n",
    "    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "    dist_from_center = dist_from_center.permute(2,0,1) \n",
    "\n",
    "    #Test case\n",
    "    #print(dist_from_center[0,0,0]) #=255\n",
    "    #print(dist_from_center[0,0,255]) #=360.624\n",
    "    #print(dist_from_center[0,255,0]) #=0\n",
    "    #print(dist_from_center[0,255,255]) #=255\n",
    "    #print(dist_from_center[0,127,127]) #=180.313 \n",
    "    \n",
    "    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \n",
    "    masks = masks[:,None,:,:].int() \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d5efb3d-3fc5-456b-afc8-40220ff5a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup evaluation\n",
    "from diffusers.utils import make_image_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torcheval.metrics import PeakSignalNoiseRatio\n",
    "#from torcheval.metrics import StructuralSimilarity\n",
    "from skimage.metrics import structural_similarity\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline, eval_dataloader, global_step, tb_summary):\n",
    "    batch = next(iter(eval_dataloader)) #TODO: Anpassen, falls grösseres evaluation set. Evt. anpassen für accelerate.\n",
    "    clean_images = batch[\"gt_image\"]\n",
    "\n",
    "    generator = torch.cuda.manual_seed(config.seed) if torch.cuda.is_available() else torch.manual_seed(config.seed)\n",
    "\n",
    "    masks = generate_masks(n=clean_images.shape[0], generator=generator, device=clean_images.device)\n",
    "\n",
    "    voided_images = clean_images*masks \n",
    "\n",
    "    images = pipeline(\n",
    "        voided_images,\n",
    "        masks,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "        output_type=np.array,\n",
    "    ).images\n",
    "\n",
    "    images = (torch.from_numpy(images)).to(clean_images.device) \n",
    "    images = torch.permute(images, (0, 3, 1, 2)) \n",
    "\n",
    "    #PSNR metric\n",
    "    metric = PeakSignalNoiseRatio(device=clean_images.device)\n",
    "    metric.update(images, clean_images)\n",
    "    PSNR = metric.compute()\n",
    "    tb_summary.add_scalar(\"PSNR\", PSNR, global_step) \n",
    "\n",
    "    #SSIM metric\n",
    "    #metric = StructuralSimilarity(device=clean_images.device)\n",
    "    #metric.update(images, clean_images)\n",
    "    #SSIM = metric.compute()\n",
    "    #tb_summary.add_scalar(\"SSIM\", SSIM, global_step) \n",
    "    batch_size = images.shape[0]\n",
    "    mssim_sum=0\n",
    "    for idx in range(batch_size):\n",
    "        mssim = structural_similarity(\n",
    "            images[idx].detach().cpu().numpy(),\n",
    "            clean_images[idx].detach().cpu().numpy(),\n",
    "            channel_axis=0,\n",
    "            data_range=2\n",
    "        )\n",
    "        mssim_sum += mssim\n",
    "    SSIM = mssim_sum / batch_size\n",
    "    tb_summary.add_scalar(\"SSIM\", SSIM, global_step)\n",
    "\n",
    "    print(\"PSNR: \", PSNR)\n",
    "    print(\"SSIM: \", SSIM)\n",
    "    print(\"global_step: \", global_step)\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    images = [to_pil_image(x) for x in images]\n",
    "    image_grid = make_image_grid(images, rows=2, cols=2)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/inpainted_image_{epoch:04d}.png\")\n",
    "    \n",
    "    pil_voided_images = [to_pil_image(x) for x in voided_images]\n",
    "    voided_image_grid = make_image_grid(pil_voided_images, rows=2, cols=2)\n",
    "    voided_image_grid.save(f\"{test_dir}/voided_image_{epoch:04d}.png\")\n",
    "    \n",
    "    print(\"image saved\")\n",
    "\n",
    "#TODO: As soon as I evaluate metrics I need to adjust the evaluate function to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d20651-5544-4864-822a-b30dcb5d1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_logs(tb_summary):\n",
    "    #log at tensorboard\n",
    "    tb_summary.add_scalar(\"image_size\", config.image_size, 0)\n",
    "    tb_summary.add_scalar(\"train_batch_size\", config.train_batch_size, 0)\n",
    "    tb_summary.add_scalar(\"eval_batch_size\", config.eval_batch_size, 0)\n",
    "    tb_summary.add_scalar(\"num_epochs\", config.num_epochs, 0)\n",
    "    tb_summary.add_scalar(\"learning_rate\", config.learning_rate, 0)\n",
    "    tb_summary.add_scalar(\"lr_warmup_steps\", config.lr_warmup_steps, 0)\n",
    "    tb_summary.add_scalar(\"evaluate_epochs\", config.evaluate_epochs, 0)\n",
    "    tb_summary.add_text(\"mixed_precision\", config.mixed_precision, 0) \n",
    "    tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) \n",
    "    tb_summary.add_text(\"lr_scheduler\", \"cosine_schedule_with_warmup\", 0)\n",
    "    tb_summary.add_text(\"model\", \"UNet2DModel\", 0) \n",
    "    tb_summary.add_text(\"inference_pipeline\", \"DDIMPipeline\", 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6078de-93ba-4c23-8f45-6b2e737cd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import Accelerator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, lr_scheduler):\n",
    "    # setup training environment\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        project_dir=os.path.join(config.output_dir, \"tensorboard\"),\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        #setup tensorboard\n",
    "        tb_summary = SummaryWriter(config.output_dir, purge_step=0)\n",
    "        meta_logs(tb_summary)\n",
    "        \n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True) \n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    train_dataloader = get_dataloader(dataset = datasetTrain, batch_size = config.train_batch_size, shuffle=True) \n",
    "    eval_dataloader = get_dataloader(dataset = datasetEvaluation, batch_size = config.eval_batch_size, shuffle=False) \n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)  \n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #eval test case\n",
    "    if accelerator.is_main_process:\n",
    "        epoch=0\n",
    "        model.eval()\n",
    "        pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)  \n",
    "        evaluate(config, epoch, pipeline, eval_dataloader, global_step, tb_summary)\n",
    "    \n",
    "    \n",
    "    # Train model\n",
    "    model.train()\n",
    "    for epoch in range(config.num_epochs): \n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process) \n",
    "        progress_bar.set_description(f\"Epoch {epoch}\") \n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            \n",
    "            clean_images = batch[\"gt_image\"]\n",
    "            segmentations = batch[\"segm\"]\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            #create mask for each img\n",
    "            masks = generate_masks(n = clean_images.shape[0], device=clean_images.device)\n",
    "            \n",
    "            #create voided img\n",
    "            voided_images = clean_images*masks\n",
    "\n",
    "            # Add noise to the voided images according to the noise magnitude at each timestep (forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) \n",
    "\n",
    "            # concatenate noisy_images, voided_images and mask\n",
    "            input=torch.cat((noisy_images, voided_images, masks), dim=1)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(input, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "                #log gradient norm \n",
    "                parameters = [p for p in model.parameters() if p.grad is not None and p.requires_grad]\n",
    "                if len(parameters) == 0:\n",
    "                    total_norm = 0.0\n",
    "                else: \n",
    "                    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()).cpu() for p in parameters]), 2.0).item()\n",
    "\n",
    "                #do learning step\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # save logs\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {\"loss\": loss.cpu().detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"total_norm\": total_norm, \"step\": global_step}\n",
    "                tb_summary.add_scalar(\"loss\", logs[\"loss\"], global_step)\n",
    "                tb_summary.add_scalar(\"lr\", logs[\"lr\"], global_step) \n",
    "                tb_summary.add_scalar(\"total_norm\", logs[\"total_norm\"], global_step) \n",
    "            \n",
    "                progress_bar.set_postfix(**logs)\n",
    "                #accelerator.log(logs, step=global_step)\n",
    "            global_step += 1 \n",
    "\n",
    "        # After a certain number of epochs it samples some images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            model.eval()\n",
    "            pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler) \n",
    "    \n",
    "            if (epoch) % config.evaluate_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline, eval_dataloader, global_step, tb_summary)\n",
    "    \n",
    "            if (epoch) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1: \n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960f42e5-88c7-4298-8ad7-2b7b4555a442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddcbf1d4f15431184d05c84a7094959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m (config, model, noise_scheduler, optimizer, lr_scheduler)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#notebook_launcher(train_loop, args, num_processes=config.num_gpu)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#notebook_launcher(train_loop, args, num_processes=1)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(config, model, noise_scheduler, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     39\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m DDIMInpaintPipeline(unet\u001b[38;5;241m=\u001b[39maccelerator\u001b[38;5;241m.\u001b[39munwrap_model(model), scheduler\u001b[38;5;241m=\u001b[39mnoise_scheduler)  \n\u001b[0;32m---> 40\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(config, epoch, pipeline, eval_dataloader, global_step)\u001b[0m\n\u001b[1;32m     15\u001b[0m masks \u001b[38;5;241m=\u001b[39m generate_masks(n\u001b[38;5;241m=\u001b[39mclean_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], generator\u001b[38;5;241m=\u001b[39mgenerator, device\u001b[38;5;241m=\u001b[39mclean_images\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m voided_images \u001b[38;5;241m=\u001b[39m clean_images\u001b[38;5;241m*\u001b[39mmasks \n\u001b[0;32m---> 19\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoided_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m     27\u001b[0m images \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mfrom_numpy(images))\u001b[38;5;241m.\u001b[39mto(clean_images\u001b[38;5;241m.\u001b[39mdevice) \n\u001b[1;32m     28\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(images, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 107\u001b[0m, in \u001b[0;36mDDIMInpaintPipeline.__call__\u001b[0;34m(self, voided_imgs, masks, batch_size, generator, eta, num_inference_steps, use_clipped_model_output, output_type, return_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((image, voided_imgs, masks), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mtimesteps):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# 1. predict noise model_output\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# 2. predict previous mean of image x_t-1 and add variance depending on eta\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# eta corresponds to η in paper and should be between [0, 1]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# do x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    113\u001b[0m         model_output, t, image, eta\u001b[38;5;241m=\u001b[39meta, use_clipped_model_output\u001b[38;5;241m=\u001b[39muse_clipped_model_output, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m    114\u001b[0m     )\u001b[38;5;241m.\u001b[39mprev_sample\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/diffusers/models/unets/unet_2d.py:329\u001b[0m, in \u001b[0;36mUNet2DModel.forward\u001b[0;34m(self, sample, timestep, class_labels, return_dict)\u001b[0m\n\u001b[1;32m    327\u001b[0m         sample, skip_sample \u001b[38;5;241m=\u001b[39m upsample_block(sample, res_samples, emb, skip_sample)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# 6. post-process\u001b[39;00m\n\u001b[1;32m    332\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_norm_out(sample)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:2664\u001b[0m, in \u001b[0;36mUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2660\u001b[0m             hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   2661\u001b[0m                 create_custom_forward(resnet), hidden_states, temb\n\u001b[1;32m   2662\u001b[0m             )\n\u001b[1;32m   2663\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2664\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2667\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/diffusers/models/resnet.py:356\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m temb\n\u001b[0;32m--> 356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding_norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_shift\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/normalization.py:287\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2561\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2560\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# If run from a jupyter notebook then uncomment the two lines and remove last line\n",
    "args = (config, model, noise_scheduler, optimizer, lr_scheduler)\n",
    "#notebook_launcher(train_loop, args, num_processes=config.num_gpu)\n",
    "#notebook_launcher(train_loop, args, num_processes=1)\n",
    "\n",
    "train_loop(config, model, noise_scheduler, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf38b5e5-8e55-4e55-b756-4bfd75248e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_filling_conditioned_lesions.ipynb to script\n",
      "[NbConvertApp] Writing 29844 bytes to lesion_filling_conditioned_lesions.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix\n",
    "!jupyter nbconvert --to script \"lesion_filling_conditioned_lesions.ipynb\"\n",
    "filename=\"lesion_filling_conditioned_lesions.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
