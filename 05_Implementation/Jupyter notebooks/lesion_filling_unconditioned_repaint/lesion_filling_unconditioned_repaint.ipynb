{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b277e45-ea4c-4639-9398-d22356d0222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../custom_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f1e0a8-8f60-4b1b-94b8-8367832dccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1\n",
    "    train_batch_size = 4 \n",
    "    eval_batch_size = 4  \n",
    "    num_epochs = 500\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    evaluate_epochs = 40 # anpassen auf Anzahl epochs\n",
    "    evaluate_num_batches = 2 # one batch needs ~130s \n",
    "    deactivate3Devaluation = True\n",
    "    evaluate_3D_epochs = 1000  # one 3D evaluation has 77 slices and needs 166min\n",
    "    save_model_epochs = 300\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-repaint\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./dataset_train/imgs\"\n",
    "    segm_train_path = \"./dataset_train/segm\"\n",
    "    masks_train_path = \"./dataset_train/masks\"\n",
    "    dataset_eval_path = \"./dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./dataset_eval/segm\"\n",
    "    masks_eval_path = \"./dataset_eval/masks\"  \n",
    "    train_only_connected_masks=False  # No Training with lesion masks\n",
    "    eval_only_connected_masks=False \n",
    "    num_inference_steps=50\n",
    "    mode = \"train\" # train / eval\n",
    "    debug = True\n",
    "    jump_length=8\n",
    "    jump_n_sample=10 \n",
    "    #uniform_dataset_path = \"./uniform_dataset\"\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc472c2-c374-4859-8ed5-791ebeafbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps=5\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1 \n",
    "    config.train_only_connected_masks=False\n",
    "    config.eval_only_connected_masks=False\n",
    "    config.evaluate_num_batches=1\n",
    "    dataset_train_path = \"./dataset_eval/imgs\"\n",
    "    segm_train_path = \"./dataset_eval/segm\"\n",
    "    masks_train_path = \"./dataset_eval/masks\"  \n",
    "    config.jump_length=1\n",
    "    config.jump_n_sample=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521352b5-cf3f-41a2-b9c3-2fbf8c07c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jovyan/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0285a5c6-3169-4eeb-9dc3-afa90494b008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# create dataset\\nimport torch \\nfrom torch.utils.data import Dataset\\nfrom torch.nn import functional as F\\nfrom pathlib import Path\\nimport nibabel as nib\\nimport numpy as np\\nfrom math import floor, ceil\\n\\nclass DatasetMRI(Dataset):\\n    \\n    Dataset for Training purposes. \\n    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\\n    \\n    Contains ground truth t1n images (gt) \\n    Args:\\n        root_dir_img: Path to img files\\n        root_dir_segm: Path to segmentation maps\\n        pad_shape: Shape the images will be transformed to\\n\\n    Raises:\\n        UserWarning: When your input images are not (256, 256, 160)\\n\\n    Returns: \\n        __getitem__: Returns a dictoinary containing:\\n            \"gt_image\": Padded and cropped version of t1n 2D slice\\n            \"t1n_path\": Path to the unpadded t1n file for this sample\\n            \"max_v\": Maximal value of t1 image (used for normalization) \\n    \\n\\n    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256)):\\n        #Initialize variables\\n        self.root_dir_img = root_dir_img\\n        self.root_dir_segm = root_dir_segm\\n        self.pad_shape = pad_shape \\n        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\\n        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\\n        #define offsets between first and last segmented slices and the slices to be used for training\\n        bottom_offset=60 \\n        top_offset=20\\n\\n        #go through all 3D imgs\\n        idx=0\\n        self.idx_to_2D_slice = dict()\\n        for j, path in enumerate(self.list_paths_segm):\\n            t1n_segm = nib.load(path)\\n            t1n_3d = t1n_segm.get_fdata()\\n\\n            #get first slice with segmented content and add offset\\n            i=0\\n            while(not t1n_3d[:,i,:].any()):\\n                i+=1\\n            bottom=i+bottom_offset\\n\\n            #get last slice with segmented content and add offset\\n            i=t1n_3d.shape[1]-1\\n            while(not t1n_3d[:,i,:].any()):\\n                i-=1\\n            top=i-top_offset\\n\\n            #Add all slices between desired top and bottom slice to dataset\\n            for i in np.arange(top-bottom):\\n                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j],bottom+i)\\n                idx+=1 \\n\\n    def __len__(self): \\n        return len(self.idx_to_2D_slice.keys()) \\n\\n    def preprocess(self, t1n: np.ndarray):\\n        \\n        Transforms the images to a more unified format.\\n        Normalizes to -1,1. Pad and crop to bounding box.\\n        \\n        Args:\\n            t1n (np.ndarray): t1n from t1n file (ground truth).\\n\\n        Raises:\\n            UserWarning: When your input images are not (256, 256, 160)\\n\\n        Returns:\\n            t1n: The padded and cropped version of t1n.\\n            t1n_max_v: Maximal value of t1n image (used for normalization).\\n        \\n\\n        #Size assertions\\n        reference_shape = (256,256,160)\\n        if t1n.shape != reference_shape:\\n            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\\n\\n        #Normalize the image to [0,1]\\n        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\\n        t1n_max_v = np.max(t1n)\\n        t1n /= t1n_max_v\\n\\n        #pad to bounding box\\n        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\\n        t1n = torch.Tensor(t1n)\\n        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\\n        d_max, w_max, h_max = size\\n        d_pad = max((d_max - d) / 2, 0)\\n        w_pad = max((w_max - w) / 2, 0)\\n        h_pad = max((h_max - h) / 2, 0)\\n        padding = (\\n            int(floor(h_pad)),\\n            int(ceil(h_pad)),\\n            int(floor(w_pad)),\\n            int(ceil(w_pad)),\\n            int(floor(d_pad)),\\n            int(ceil(d_pad)),\\n        )\\n        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \\n\\n        #map images from [0,1] to [-1,1]\\n        t1n = (t1n*2) - 1\\n\\n        return t1n, t1n_max_v\\n\\n    def __getitem__(self, idx):\\n        t1n_path = self.idx_to_2D_slice[idx][0]\\n        slice_idx = self.idx_to_2D_slice[idx][1]\\n        t1n_img = nib.load(t1n_path)\\n        t1n = t1n_img.get_fdata()\\n        \\n        # preprocess data\\n        t1n, t1n_max_v = self.preprocess(t1n) # around 0.2s on local machine\\n        \\n        # get 2D slice from 3D\\n        t1n_slice = t1n[:,slice_idx,:] \\n        \\n        # Output data\\n        sample_dict = {\\n            \"gt_image\": t1n_slice.unsqueeze(0),\\n            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\\n            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization \\n        }\\n        return sample_dict \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# create dataset\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "class DatasetMRI(Dataset):\n",
    "    \n",
    "    Dataset for Training purposes. \n",
    "    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\n",
    "    \n",
    "    Contains ground truth t1n images (gt) \n",
    "    Args:\n",
    "        root_dir_img: Path to img files\n",
    "        root_dir_segm: Path to segmentation maps\n",
    "        pad_shape: Shape the images will be transformed to\n",
    "\n",
    "    Raises:\n",
    "        UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "    Returns: \n",
    "        __getitem__: Returns a dictoinary containing:\n",
    "            \"gt_image\": Padded and cropped version of t1n 2D slice\n",
    "            \"t1n_path\": Path to the unpadded t1n file for this sample\n",
    "            \"max_v\": Maximal value of t1 image (used for normalization) \n",
    "    \n",
    "\n",
    "    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256)):\n",
    "        #Initialize variables\n",
    "        self.root_dir_img = root_dir_img\n",
    "        self.root_dir_segm = root_dir_segm\n",
    "        self.pad_shape = pad_shape \n",
    "        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\n",
    "        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\n",
    "        #define offsets between first and last segmented slices and the slices to be used for training\n",
    "        bottom_offset=60 \n",
    "        top_offset=20\n",
    "\n",
    "        #go through all 3D imgs\n",
    "        idx=0\n",
    "        self.idx_to_2D_slice = dict()\n",
    "        for j, path in enumerate(self.list_paths_segm):\n",
    "            t1n_segm = nib.load(path)\n",
    "            t1n_3d = t1n_segm.get_fdata()\n",
    "\n",
    "            #get first slice with segmented content and add offset\n",
    "            i=0\n",
    "            while(not t1n_3d[:,i,:].any()):\n",
    "                i+=1\n",
    "            bottom=i+bottom_offset\n",
    "\n",
    "            #get last slice with segmented content and add offset\n",
    "            i=t1n_3d.shape[1]-1\n",
    "            while(not t1n_3d[:,i,:].any()):\n",
    "                i-=1\n",
    "            top=i-top_offset\n",
    "\n",
    "            #Add all slices between desired top and bottom slice to dataset\n",
    "            for i in np.arange(top-bottom):\n",
    "                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j],bottom+i)\n",
    "                idx+=1 \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.idx_to_2D_slice.keys()) \n",
    "\n",
    "    def preprocess(self, t1n: np.ndarray):\n",
    "        \n",
    "        Transforms the images to a more unified format.\n",
    "        Normalizes to -1,1. Pad and crop to bounding box.\n",
    "        \n",
    "        Args:\n",
    "            t1n (np.ndarray): t1n from t1n file (ground truth).\n",
    "\n",
    "        Raises:\n",
    "            UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "        Returns:\n",
    "            t1n: The padded and cropped version of t1n.\n",
    "            t1n_max_v: Maximal value of t1n image (used for normalization).\n",
    "        \n",
    "\n",
    "        #Size assertions\n",
    "        reference_shape = (256,256,160)\n",
    "        if t1n.shape != reference_shape:\n",
    "            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\n",
    "\n",
    "        #Normalize the image to [0,1]\n",
    "        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\n",
    "        t1n_max_v = np.max(t1n)\n",
    "        t1n /= t1n_max_v\n",
    "\n",
    "        #pad to bounding box\n",
    "        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\n",
    "        t1n = torch.Tensor(t1n)\n",
    "        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\n",
    "        d_max, w_max, h_max = size\n",
    "        d_pad = max((d_max - d) / 2, 0)\n",
    "        w_pad = max((w_max - w) / 2, 0)\n",
    "        h_pad = max((h_max - h) / 2, 0)\n",
    "        padding = (\n",
    "            int(floor(h_pad)),\n",
    "            int(ceil(h_pad)),\n",
    "            int(floor(w_pad)),\n",
    "            int(ceil(w_pad)),\n",
    "            int(floor(d_pad)),\n",
    "            int(ceil(d_pad)),\n",
    "        )\n",
    "        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \n",
    "\n",
    "        #map images from [0,1] to [-1,1]\n",
    "        t1n = (t1n*2) - 1\n",
    "\n",
    "        return t1n, t1n_max_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1n_path = self.idx_to_2D_slice[idx][0]\n",
    "        slice_idx = self.idx_to_2D_slice[idx][1]\n",
    "        t1n_img = nib.load(t1n_path)\n",
    "        t1n = t1n_img.get_fdata()\n",
    "        \n",
    "        # preprocess data\n",
    "        t1n, t1n_max_v = self.preprocess(t1n) # around 0.2s on local machine\n",
    "        \n",
    "        # get 2D slice from 3D\n",
    "        t1n_slice = t1n[:,slice_idx,:] \n",
    "        \n",
    "        # Output data\n",
    "        sample_dict = {\n",
    "            \"gt_image\": t1n_slice.unsqueeze(0),\n",
    "            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\n",
    "            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization \n",
    "        }\n",
    "        return sample_dict \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2470925bbd9c4a2ab9ef30cd1c3575dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2330cfcaf7924832bdd2bfe5894e540f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f1f2b86dc94ffda46d59b9f6b27091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'#create dataset\\ndatasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256)) # TODO: check shape \\n\\nprint(f\"Dataset size: {len(datasetEvaluation)}\")\\nprint(f\"\\tImage shape: {datasetEvaluation[0][\\'gt_image\\'].shape}\")\\nprint(f\"Evaluation Data: {list(datasetEvaluation[0].keys())}\") '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DatasetMRI2D import DatasetMRI2D\n",
    "from DatasetMRI3D import DatasetMRI3D\n",
    "from pathlib import Path\n",
    "\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI2D(Path(config.dataset_train_path), Path(config.segm_train_path), only_connected_masks=config.train_only_connected_masks)\n",
    "datasetEvaluation = DatasetMRI2D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "dataset3DEvaluation = DatasetMRI3D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "\n",
    "\"\"\"#create dataset\n",
    "datasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256)) # TODO: check shape \n",
    "\n",
    "print(f\"Dataset size: {len(datasetEvaluation)}\")\n",
    "print(f\"\\tImage shape: {datasetEvaluation[0]['gt_image'].shape}\")\n",
    "print(f\"Evaluation Data: {list(datasetEvaluation[0].keys())}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2e771e-6527-4d9d-898f-2fcfc6b5cb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from torch.utils.data import DataLoader\\n\\n# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\\ndef get_dataloader(dataset, batch_size, shuffle):\\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1188218d-a16d-4c0a-896f-e47a140bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=config.channels,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.model = \"UNet2DModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a40c63-4ba1-44aa-8c62-7f15ce816225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "config.noise_scheduler = \"DDIMScheduler(num_train_timesteps=1000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f42d0a-9ebb-4437-b180-12b79af3c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "config.lr_scheduler = \"cosine_schedule_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ff284a-4306-4677-829e-98fef153fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 15:35:57.685437: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 15:35:57.761171: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 15:35:57.761217: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 15:35:57.761250: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 15:35:57.770724: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 15:35:57.771461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 15:35:59.946724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from TrainingUnconditional import TrainingUnconditional\n",
    "from diffusers import RePaintPipeline\n",
    "\n",
    "config.conditional_data = \"None\"\n",
    "\n",
    "args = {\"config\": config, \"model\": model, \"noise_scheduler\": noise_scheduler, \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"datasetTrain\": datasetTrain, \"datasetEvaluation\": datasetEvaluation, \"dataset3DEvaluation\": dataset3DEvaluation, \"deactivate3Devaluation\": config.deactivate3Devaluation} \n",
    "trainingRepaint = TrainingUnconditional(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bac4e0a-2837-4217-a97c-ce940bdc2151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd997e0e46c4632b98e628b412a6a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/diffusers/pipelines/deprecated/repaint/pipeline_repaint.py:35: FutureWarning: The preprocess method is deprecated and will be removed in diffusers 1.0.0. Please use VaeImageProcessor.preprocess(...) instead\n",
      "  deprecate(\"preprocess\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2b8730f5fc4d9a8d15efcbc97c383b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inapint:  tensor(0.9764) tensor(0.)\n",
      "clean:  tensor(0.8015) tensor(-1.)\n",
      "SSIM_global:  -0.23556962609291077\n",
      "PSNR_global:  6.843437194824219\n",
      "MSE_global:  0.8274014951395969\n",
      "global_step:  1\n",
      "image saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b7b152698443ca924191bfc3b25813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fa87fbe8750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.mode == \"train\":\n",
    "    trainingRepaint.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7971-c0ac-461d-9fcf-78481e7c9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"eval\":\n",
    "    pipeline = RePaintPipeline.from_pretrained(config.output_dir) \n",
    "    trainingRepaint.evaluate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e5c59-4c1c-4fc9-8ffc-4fddca8e2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be170a32-fcb6-41b1-b6ea-14b77eb7b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_filling_unconditioned_repaint.ipynb to script\n",
      "[NbConvertApp] Writing 11376 bytes to lesion_filling_unconditioned_repaint.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix \n",
    "import os\n",
    "\n",
    "!jupyter nbconvert --to script \"lesion_filling_unconditioned_repaint.ipynb\"\n",
    "filename=\"lesion_filling_unconditioned_repaint.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
