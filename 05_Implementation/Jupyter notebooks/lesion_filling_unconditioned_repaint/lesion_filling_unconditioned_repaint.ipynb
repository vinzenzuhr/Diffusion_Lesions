{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77b51cd-dfaa-44b4-b1f6-a36e07016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1\n",
    "    train_batch_size = 2 # 16\n",
    "    eval_batch_size = 2  # 2 x 2 GPU = 4 \n",
    "    num_epochs = 600\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    evaluate_epochs = 30\n",
    "    save_model_epochs = 300\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-repaint\"  # the model name locally and on the HF Hub\n",
    "    dataset_eval_path = \"./dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./dataset_eval/segm\"\n",
    "    pretrained_path = \"./pretrained\"\n",
    "    num_gpu=2\n",
    "    #uniform_dataset_path = \"./uniform_dataset\"\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0285a5c6-3169-4eeb-9dc3-afa90494b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "class DatasetMRI(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Training purposes. \n",
    "    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\n",
    "    \n",
    "    Contains ground truth t1n images (gt) \n",
    "    Args:\n",
    "        root_dir_img: Path to img files\n",
    "        root_dir_segm: Path to segmentation maps\n",
    "        pad_shape: Shape the images will be transformed to\n",
    "\n",
    "    Raises:\n",
    "        UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "    Returns: \n",
    "        __getitem__: Returns a dictoinary containing:\n",
    "            \"gt_image\": Padded and cropped version of t1n 2D slice\n",
    "            \"t1n_path\": Path to the unpadded t1n file for this sample\n",
    "            \"max_v\": Maximal value of t1 image (used for normalization) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256)):\n",
    "        #Initialize variables\n",
    "        self.root_dir_img = root_dir_img\n",
    "        self.root_dir_segm = root_dir_segm\n",
    "        self.pad_shape = pad_shape \n",
    "        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\n",
    "        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\n",
    "        #define offsets between first and last segmented slices and the slices to be used for training\n",
    "        bottom_offset=60 \n",
    "        top_offset=20\n",
    "\n",
    "        #go through all 3D imgs\n",
    "        idx=0\n",
    "        self.idx_to_2D_slice = dict()\n",
    "        for j, path in enumerate(self.list_paths_segm):\n",
    "            t1n_segm = nib.load(path)\n",
    "            t1n_3d = t1n_segm.get_fdata()\n",
    "\n",
    "            #get first slice with segmented content and add offset\n",
    "            i=0\n",
    "            while(not t1n_3d[:,i,:].any()):\n",
    "                i+=1\n",
    "            bottom=i+bottom_offset\n",
    "\n",
    "            #get last slice with segmented content and add offset\n",
    "            i=t1n_3d.shape[1]-1\n",
    "            while(not t1n_3d[:,i,:].any()):\n",
    "                i-=1\n",
    "            top=i-top_offset\n",
    "\n",
    "            #Add all slices between desired top and bottom slice to dataset\n",
    "            for i in np.arange(top-bottom):\n",
    "                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j],bottom+i)\n",
    "                idx+=1 \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.idx_to_2D_slice.keys()) \n",
    "\n",
    "    def preprocess(self, t1n: np.ndarray):\n",
    "        \"\"\"\n",
    "        Transforms the images to a more unified format.\n",
    "        Normalizes to -1,1. Pad and crop to bounding box.\n",
    "        \n",
    "        Args:\n",
    "            t1n (np.ndarray): t1n from t1n file (ground truth).\n",
    "\n",
    "        Raises:\n",
    "            UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "        Returns:\n",
    "            t1n: The padded and cropped version of t1n.\n",
    "            t1n_max_v: Maximal value of t1n image (used for normalization).\n",
    "        \"\"\"\n",
    "\n",
    "        #Size assertions\n",
    "        reference_shape = (256,256,160)\n",
    "        if t1n.shape != reference_shape:\n",
    "            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\n",
    "\n",
    "        #Normalize the image to [0,1]\n",
    "        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\n",
    "        t1n_max_v = np.max(t1n)\n",
    "        t1n /= t1n_max_v\n",
    "\n",
    "        #pad to bounding box\n",
    "        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\n",
    "        t1n = torch.Tensor(t1n)\n",
    "        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\n",
    "        d_max, w_max, h_max = size\n",
    "        d_pad = max((d_max - d) / 2, 0)\n",
    "        w_pad = max((w_max - w) / 2, 0)\n",
    "        h_pad = max((h_max - h) / 2, 0)\n",
    "        padding = (\n",
    "            int(floor(h_pad)),\n",
    "            int(ceil(h_pad)),\n",
    "            int(floor(w_pad)),\n",
    "            int(ceil(w_pad)),\n",
    "            int(floor(d_pad)),\n",
    "            int(ceil(d_pad)),\n",
    "        )\n",
    "        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \n",
    "\n",
    "        #map images from [0,1] to [-1,1]\n",
    "        t1n = (t1n*2) - 1\n",
    "\n",
    "        return t1n, t1n_max_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1n_path = self.idx_to_2D_slice[idx][0]\n",
    "        slice_idx = self.idx_to_2D_slice[idx][1]\n",
    "        t1n_img = nib.load(t1n_path)\n",
    "        t1n = t1n_img.get_fdata()\n",
    "        \n",
    "        # preprocess data\n",
    "        t1n, t1n_max_v = self.preprocess(t1n) # around 0.2s on local machine\n",
    "        \n",
    "        # get 2D slice from 3D\n",
    "        t1n_slice = t1n[:,slice_idx,:] \n",
    "        \n",
    "        # Output data\n",
    "        sample_dict = {\n",
    "            \"gt_image\": t1n_slice.unsqueeze(0),\n",
    "            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\n",
    "            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization \n",
    "        }\n",
    "        return sample_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 223\n",
      "\tImage shape: torch.Size([1, 256, 256])\n",
      "Evaluation Data: ['gt_image', 't1n_path', 'max_v']\n"
     ]
    }
   ],
   "source": [
    "#create dataset\n",
    "datasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256)) # TODO: check shape \n",
    "\n",
    "print(f\"Dataset size: {len(datasetEvaluation)}\")\n",
    "print(f\"\\tImage shape: {datasetEvaluation[0]['gt_image'].shape}\")\n",
    "print(f\"Evaluation Data: {list(datasetEvaluation[0].keys())}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b2e771e-6527-4d9d-898f-2fcfc6b5cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e48ed1-df20-4cdb-9191-75c7549af68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdee4a9543a4a018030c7ee445dd4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from diffusers import DDIMPipeline\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "pipe = DDIMPipeline.from_pretrained(config.pretrained_path)\n",
    "model=pipe.unet\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c682ff-3586-46d9-9bf4-a75e35c6cba9",
   "metadata": {},
   "source": [
    "### RePaint Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b631d26-64e0-4867-8028-37d123cc4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(n, device, generator=None):\n",
    "    #create circular mask with random center around the center point of the pictures and a radius between 3 and 50 pixels\n",
    "    center=torch.normal(mean=config.image_size/2, std=30, size=(n,2), generator=generator, device=device) # 30 is chosen by inspection\n",
    "    low=3   \n",
    "    high=50\n",
    "    radius=torch.rand(n, generator=generator, device=device)*(high-low)+low # get radius between 3 and 50 from uniform distribution \n",
    "\n",
    "    #Test case\n",
    "    #center=torch.tensor([[0,255],[0,255]]) \n",
    "    #radius=torch.tensor([2,2])\n",
    "    \n",
    "    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "    dist_from_center = dist_from_center.permute(2,0,1) \n",
    "\n",
    "    #Test case\n",
    "    #print(dist_from_center[0,0,0]) #=255\n",
    "    #print(dist_from_center[0,0,255]) #=360.624\n",
    "    #print(dist_from_center[0,255,0]) #=0\n",
    "    #print(dist_from_center[0,255,255]) #=255\n",
    "    #print(dist_from_center[0,127,127]) #=180.313 \n",
    "    \n",
    "    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \n",
    "    masks = masks[:,None,:,:].int() \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0c27d1-2c78-4a05-8962-85f1b6a50216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup evaluation\n",
    "from diffusers import DDIMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def evaluate(config, pipeline, eval_dataloader, accelerator):\n",
    "    batch = next(iter(eval_dataloader)) #TODO: Anpassen, falls grösseres evaluation set. Evt. anpassen für accelerate.\n",
    "    clean_images = batch[\"gt_image\"]\n",
    "    \n",
    "    generator = torch.cuda.manual_seed(config.seed) if torch.cuda.is_available() else torch.manual_seed(config.seed)\n",
    "    masks = generate_masks(n=clean_images.shape[0], generator=generator, device=clean_images.device)\n",
    "    voided_images = clean_images*masks \n",
    "\n",
    "    images = pipeline(voided_images, masks, generator=torch.manual_seed(config.seed), output_type=np.array, jump_length=8).images\n",
    "    images = (torch.from_numpy(images)).to(clean_images.device) \n",
    "    \n",
    "\n",
    "    #collect all images from the different processes/GPUs\n",
    "    clean_images  = accelerator.gather(clean_images)\n",
    "    images = accelerator.gather(images)\n",
    "    voided_images = accelerator.gather(voided_images)\n",
    "\n",
    "    if accelerator.is_main_process: \n",
    "        #bevor ich SSIM etc. einbaue muss ich di permutation von images vorher machen. Siehe 3 Zeile weiter unten\n",
    "\n",
    "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True) \n",
    "        \n",
    "        pil_images = [to_pil_image(x.permute(2,0,1)) for x in images]\n",
    "        image_grid = make_image_grid(pil_images, rows=2, cols=2)\n",
    "        image_grid.save(f\"{test_dir}/inpainted_image.png\")\n",
    "        \n",
    "        pil_voided_images = [to_pil_image(x) for x in voided_images]\n",
    "        voided_image_grid = make_image_grid(pil_voided_images, rows=2, cols=2)\n",
    "        voided_image_grid.save(f\"{test_dir}/voided_image.png\")    \n",
    "\n",
    "        pil_clean_images = [to_pil_image(x) for x in clean_images]\n",
    "        clean_image_grid = make_image_grid(pil_clean_images, rows=2, cols=2)\n",
    "        clean_image_grid.save(f\"{test_dir}/clean_image.png\")   \n",
    "        \n",
    "        print(\"image saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfb5413-55b1-44fe-8c8d-c4093c672cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import RePaintPipeline, RePaintScheduler  \n",
    "from accelerate import Accelerator, PartialState\n",
    "\n",
    "accelerator = Accelerator()\n",
    "distributed_state = PartialState()\n",
    "\n",
    "eval_dataloader = get_dataloader(datasetEvaluation, config.eval_batch_size, shuffle=False)\n",
    "model.eval()\n",
    "pipeline = RePaintPipeline(unet=model, scheduler=RePaintScheduler())\n",
    "pipeline.to(distributed_state.device) \n",
    "\n",
    "pipeline, eval_dataloader = accelerator.prepare(\n",
    "        pipeline, eval_dataloader\n",
    "    )\n",
    "\n",
    "evaluate(config, pipeline, eval_dataloader, accelerator)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be170a32-fcb6-41b1-b6ea-14b77eb7b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_filling_unconditioned_repaint.ipynb to script\n",
      "[NbConvertApp] Writing 11148 bytes to lesion_filling_unconditioned_repaint.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix \n",
    "import os\n",
    "\n",
    "!jupyter nbconvert --to script \"lesion_filling_unconditioned_repaint.ipynb\"\n",
    "filename=\"lesion_filling_unconditioned_repaint.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
