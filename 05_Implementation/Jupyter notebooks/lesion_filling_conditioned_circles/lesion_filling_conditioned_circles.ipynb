{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcd1318-8fee-4938-b366-05698a04bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../custom_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77b51cd-dfaa-44b4-b1f6-a36e07016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # TODO: the generated image resolution\n",
    "    channels = 1 \n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "    num_epochs = 90 #600\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 100 #500\n",
    "    evaluate_epochs = 20 #30\n",
    "    evaluate_num_batches = 20 # one batch needs ~15s.  \n",
    "    evaluate_3D_epochs = 1000  # one 3D evaluation needs ~20min\n",
    "    save_model_epochs = 60 # 300\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-filling-256-cond-circle\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./dataset_train/imgs\"\n",
    "    segm_train_path = \"./dataset_train/segm\"\n",
    "    masks_train_path = \"./dataset_train/masks\"\n",
    "    dataset_eval_path = \"./dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./dataset_eval/segm\"\n",
    "    masks_eval_path = \"./dataset_eval/masks\" \n",
    "    train_only_connected_masks=False # No Training with lesion masks\n",
    "    eval_only_connected_masks=False \n",
    "    num_inference_steps=50\n",
    "    mode = \"train\" # train / eval\n",
    "    debug = True\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    #hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    #hub_private_repo = False\n",
    "    #overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5a5223-3c9a-4287-b44a-1a6c1a411538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps=5\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1 \n",
    "    config.train_only_connected_masks=False\n",
    "    config.eval_only_connected_masks=False\n",
    "    config.evaluate_num_batches=1\n",
    "    dataset_train_path = \"./dataset_eval/imgs\"\n",
    "    segm_train_path = \"./dataset_eval/segm\"\n",
    "    masks_train_path = \"./dataset_eval/masks\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e2ffca-7e8c-43d4-8e13-ba1900e38eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jovyan/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2e0607-1880-4546-85f7-fad739391467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#log at tensorboard\\ntb_summary.add_scalar(\"image_size\", config.image_size, 0)\\ntb_summary.add_scalar(\"train_batch_size\", config.train_batch_size, 0)\\ntb_summary.add_scalar(\"eval_batch_size\", config.eval_batch_size, 0)\\ntb_summary.add_scalar(\"num_epochs\", config.num_epochs, 0)\\ntb_summary.add_scalar(\"learning_rate\", config.learning_rate, 0)\\ntb_summary.add_scalar(\"lr_warmup_steps\", config.lr_warmup_steps, 0)\\ntb_summary.add_scalar(\"evaluate_epochs\", config.evaluate_epochs, 0)\\ntb_summary.add_text(\"mixed_precision\", config.mixed_precision, 0) \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#log at tensorboard\n",
    "tb_summary.add_scalar(\"image_size\", config.image_size, 0)\n",
    "tb_summary.add_scalar(\"train_batch_size\", config.train_batch_size, 0)\n",
    "tb_summary.add_scalar(\"eval_batch_size\", config.eval_batch_size, 0)\n",
    "tb_summary.add_scalar(\"num_epochs\", config.num_epochs, 0)\n",
    "tb_summary.add_scalar(\"learning_rate\", config.learning_rate, 0)\n",
    "tb_summary.add_scalar(\"lr_warmup_steps\", config.lr_warmup_steps, 0)\n",
    "tb_summary.add_scalar(\"evaluate_epochs\", config.evaluate_epochs, 0)\n",
    "tb_summary.add_text(\"mixed_precision\", config.mixed_precision, 0) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0285a5c6-3169-4eeb-9dc3-afa90494b008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create dataset\\nfrom torch.utils.data import Dataset\\nfrom torch.nn import functional as F\\nfrom pathlib import Path\\nimport nibabel as nib\\nimport numpy as np\\nfrom math import floor, ceil\\n\\nclass DatasetMRI(Dataset):\\n    \\n    Dataset for Training purposes. \\n    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\\n    \\n    Contains ground truth t1n images (gt) \\n    Args:\\n        root_dir_img: Path to img files\\n        root_dir_segm: Path to segmentation maps\\n        pad_shape: Shape the images will be transformed to\\n\\n    Raises:\\n        UserWarning: When your input images are not (256, 256, 160)\\n\\n    Returns: \\n        __getitem__: Returns a dictoinary containing:\\n            \"gt_image\": Padded and cropped version of t1n 2D slice\\n            \"segm\": Segmentation of 2D slice\\n            \"t1n_path\": Path to the unpadded t1n file for this sample\\n            \"max_v\": Maximal value of t1 image (used for normalization)\\n            \\n    \\n\\n    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256), only_white_matter=False):\\n        #Initialize variables\\n        self.root_dir_img = root_dir_img\\n        self.root_dir_segm = root_dir_segm\\n        self.pad_shape = pad_shape \\n        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\\n        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\\n        #define offsets between first and last segmented slices and the slices to be used for training\\n        bottom_offset=60\\n        top_offset=20\\n\\n        #go through all 3D imgs\\n        idx=0\\n        self.idx_to_2D_slice = dict()\\n        for j, path in enumerate(self.list_paths_segm):\\n            t1n_segm = nib.load(path)\\n            t1n_3d = t1n_segm.get_fdata()\\n\\n            #transform segmentation\\n            t1n_3d = np.transpose(t1n_3d)\\n            t1n_3d = np.flip(t1n_3d, axis=1)\\n\\n            #get first slice with white matter or segmented content plus offset\\n            i=0\\n            if(only_white_matter):\\n                #while there is no white matter go to the next slide\\n                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\\n                    i += 1\\n                bottom = i\\n            else:\\n                #while there is no segmented tissue go to the next slide\\n                while(not t1n_3d[:,i,:].any()):\\n                    i += 1\\n                bottom = i + bottom_offset \\n\\n            #get last slice with white matter or segmented content minus offset\\n            i=t1n_3d.shape[1]-1\\n            if(only_white_matter):\\n                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\\n                    i -= 1\\n                top = i\\n            else:\\n                while(not t1n_3d[:,i,:].any()):\\n                    i -= 1\\n                top = i - top_offset \\n            \\n            #Add all slices between desired top and bottom slice to dataset\\n            for i in np.arange(top-bottom):\\n                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j], self.list_paths_segm[j],bottom+i, [])\\n                idx+=1   \\n\\n    def __len__(self): \\n        return len(self.idx_to_2D_slice.keys()) \\n\\n    def preprocess(self, t1n: np.ndarray):\\n        \\n        Transforms the images to a more unified format.\\n        Normalizes to -1,1. Pad and crop to bounding box.\\n        \\n        Args:\\n            t1n (np.ndarray): t1n from t1n file (ground truth).\\n\\n        Raises:\\n            UserWarning: When your input images are not (256, 256, 160)\\n\\n        Returns:\\n            t1n: The padded and cropped version of t1n.\\n            t1n_max_v: Maximal value of t1n image (used for normalization).\\n        \\n\\n        #Size assertions\\n        reference_shape = (256,256,160)\\n        if t1n.shape != reference_shape:\\n            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\\n\\n        #Normalize the image to [0,1]\\n        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\\n        t1n_max_v = np.max(t1n)\\n        t1n /= t1n_max_v\\n\\n        #pad to bounding box\\n        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\\n        t1n = torch.Tensor(t1n)\\n        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\\n        d_max, w_max, h_max = size\\n        d_pad = max((d_max - d) / 2, 0)\\n        w_pad = max((w_max - w) / 2, 0)\\n        h_pad = max((h_max - h) / 2, 0)\\n        padding = (\\n            int(floor(h_pad)),\\n            int(ceil(h_pad)),\\n            int(floor(w_pad)),\\n            int(ceil(w_pad)),\\n            int(floor(d_pad)),\\n            int(ceil(d_pad)),\\n        )\\n        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \\n\\n        #map images from [0,1] to [-1,1]\\n        t1n = (t1n*2) - 1\\n\\n        return t1n, t1n_max_v\\n\\n    def __getitem__(self, idx):\\n        t1n_path = self.idx_to_2D_slice[idx][0]\\n        segm_path = self.idx_to_2D_slice[idx][1]\\n        slice_idx = self.idx_to_2D_slice[idx][2]\\n        masks = self.idx_to_2D_slice[idx][3]\\n        \\n        t1n_img = nib.load(t1n_path)\\n        t1n = t1n_img.get_fdata()\\n        t1n_segm = nib.load(segm_path)\\n        t1n_segm_data = t1n_segm.get_fdata()\\n        \\n        t1n_segm_data = np.transpose(t1n_segm_data)\\n        t1n_segm_data = np.flip(t1n_segm_data, axis=1)\\n        \\n        # preprocess data\\n        t1n, t1n_max_v = self.preprocess(t1n)\\n        \\n        # get 2D slice from 3D\\n        t1n_slice = t1n[:,slice_idx,:] \\n        t1n_segm_slice = t1n_segm_data[:,slice_idx,:]\\n        \\n        # Output data\\n        sample_dict = {\\n            \"gt_image\": t1n_slice.unsqueeze(0),\\n            \"segm\": t1n_segm_slice,\\n            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\\n            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization\\n            \"masks\": masks, #generated masks\\n        }\\n        return sample_dict \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "class DatasetMRI(Dataset):\n",
    "    \n",
    "    Dataset for Training purposes. \n",
    "    Adapted implementation of BraTS 2023 Inpainting Challenge (https://github.com/BraTS-inpainting/2023_challenge).\n",
    "    \n",
    "    Contains ground truth t1n images (gt) \n",
    "    Args:\n",
    "        root_dir_img: Path to img files\n",
    "        root_dir_segm: Path to segmentation maps\n",
    "        pad_shape: Shape the images will be transformed to\n",
    "\n",
    "    Raises:\n",
    "        UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "    Returns: \n",
    "        __getitem__: Returns a dictoinary containing:\n",
    "            \"gt_image\": Padded and cropped version of t1n 2D slice\n",
    "            \"segm\": Segmentation of 2D slice\n",
    "            \"t1n_path\": Path to the unpadded t1n file for this sample\n",
    "            \"max_v\": Maximal value of t1 image (used for normalization)\n",
    "            \n",
    "    \n",
    "\n",
    "    def __init__(self, root_dir_img: Path, root_dir_segm: Path, pad_shape=(256,256,256), only_white_matter=False):\n",
    "        #Initialize variables\n",
    "        self.root_dir_img = root_dir_img\n",
    "        self.root_dir_segm = root_dir_segm\n",
    "        self.pad_shape = pad_shape \n",
    "        self.list_paths_t1n = list(root_dir_img.rglob(\"*.nii.gz\"))\n",
    "        self.list_paths_segm = list(root_dir_segm.rglob(\"*.nii.gz\"))\n",
    "        #define offsets between first and last segmented slices and the slices to be used for training\n",
    "        bottom_offset=60\n",
    "        top_offset=20\n",
    "\n",
    "        #go through all 3D imgs\n",
    "        idx=0\n",
    "        self.idx_to_2D_slice = dict()\n",
    "        for j, path in enumerate(self.list_paths_segm):\n",
    "            t1n_segm = nib.load(path)\n",
    "            t1n_3d = t1n_segm.get_fdata()\n",
    "\n",
    "            #transform segmentation\n",
    "            t1n_3d = np.transpose(t1n_3d)\n",
    "            t1n_3d = np.flip(t1n_3d, axis=1)\n",
    "\n",
    "            #get first slice with white matter or segmented content plus offset\n",
    "            i=0\n",
    "            if(only_white_matter):\n",
    "                #while there is no white matter go to the next slide\n",
    "                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\n",
    "                    i += 1\n",
    "                bottom = i\n",
    "            else:\n",
    "                #while there is no segmented tissue go to the next slide\n",
    "                while(not t1n_3d[:,i,:].any()):\n",
    "                    i += 1\n",
    "                bottom = i + bottom_offset \n",
    "\n",
    "            #get last slice with white matter or segmented content minus offset\n",
    "            i=t1n_3d.shape[1]-1\n",
    "            if(only_white_matter):\n",
    "                while(not np.logical_or(t1n_3d[:,i,:]==41, t1n_3d[:,i,:]==2).any()):\n",
    "                    i -= 1\n",
    "                top = i\n",
    "            else:\n",
    "                while(not t1n_3d[:,i,:].any()):\n",
    "                    i -= 1\n",
    "                top = i - top_offset \n",
    "            \n",
    "            #Add all slices between desired top and bottom slice to dataset\n",
    "            for i in np.arange(top-bottom):\n",
    "                self.idx_to_2D_slice[idx]=(self.list_paths_t1n[j], self.list_paths_segm[j],bottom+i, [])\n",
    "                idx+=1   \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.idx_to_2D_slice.keys()) \n",
    "\n",
    "    def preprocess(self, t1n: np.ndarray):\n",
    "        \n",
    "        Transforms the images to a more unified format.\n",
    "        Normalizes to -1,1. Pad and crop to bounding box.\n",
    "        \n",
    "        Args:\n",
    "            t1n (np.ndarray): t1n from t1n file (ground truth).\n",
    "\n",
    "        Raises:\n",
    "            UserWarning: When your input images are not (256, 256, 160)\n",
    "\n",
    "        Returns:\n",
    "            t1n: The padded and cropped version of t1n.\n",
    "            t1n_max_v: Maximal value of t1n image (used for normalization).\n",
    "        \n",
    "\n",
    "        #Size assertions\n",
    "        reference_shape = (256,256,160)\n",
    "        if t1n.shape != reference_shape:\n",
    "            raise UserWarning(f\"Your t1n shape is not {reference_shape}, it is {t1n.shape}\")\n",
    "\n",
    "        #Normalize the image to [0,1]\n",
    "        t1n[t1n<0] = 0 #Values below 0 are considered to be noise #TODO: Check validity\n",
    "        t1n_max_v = np.max(t1n)\n",
    "        t1n /= t1n_max_v\n",
    "\n",
    "        #pad to bounding box\n",
    "        size = self.pad_shape # shape of bounding box is (size,size,size) #TODO: Find solution for 2D\n",
    "        t1n = torch.Tensor(t1n)\n",
    "        d, w, h = t1n.shape[-3], t1n.shape[-2], t1n.shape[-1]\n",
    "        d_max, w_max, h_max = size\n",
    "        d_pad = max((d_max - d) / 2, 0)\n",
    "        w_pad = max((w_max - w) / 2, 0)\n",
    "        h_pad = max((h_max - h) / 2, 0)\n",
    "        padding = (\n",
    "            int(floor(h_pad)),\n",
    "            int(ceil(h_pad)),\n",
    "            int(floor(w_pad)),\n",
    "            int(ceil(w_pad)),\n",
    "            int(floor(d_pad)),\n",
    "            int(ceil(d_pad)),\n",
    "        )\n",
    "        t1n = F.pad(t1n, padding, value=0, mode=\"constant\") \n",
    "\n",
    "        #map images from [0,1] to [-1,1]\n",
    "        t1n = (t1n*2) - 1\n",
    "\n",
    "        return t1n, t1n_max_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1n_path = self.idx_to_2D_slice[idx][0]\n",
    "        segm_path = self.idx_to_2D_slice[idx][1]\n",
    "        slice_idx = self.idx_to_2D_slice[idx][2]\n",
    "        masks = self.idx_to_2D_slice[idx][3]\n",
    "        \n",
    "        t1n_img = nib.load(t1n_path)\n",
    "        t1n = t1n_img.get_fdata()\n",
    "        t1n_segm = nib.load(segm_path)\n",
    "        t1n_segm_data = t1n_segm.get_fdata()\n",
    "        \n",
    "        t1n_segm_data = np.transpose(t1n_segm_data)\n",
    "        t1n_segm_data = np.flip(t1n_segm_data, axis=1)\n",
    "        \n",
    "        # preprocess data\n",
    "        t1n, t1n_max_v = self.preprocess(t1n)\n",
    "        \n",
    "        # get 2D slice from 3D\n",
    "        t1n_slice = t1n[:,slice_idx,:] \n",
    "        t1n_segm_slice = t1n_segm_data[:,slice_idx,:]\n",
    "        \n",
    "        # Output data\n",
    "        sample_dict = {\n",
    "            \"gt_image\": t1n_slice.unsqueeze(0),\n",
    "            \"segm\": t1n_segm_slice,\n",
    "            \"t1n_path\": str(t1n_path),  # path to the 3D t1n file for this sample.\n",
    "            \"max_v\": t1n_max_v,  # maximal t1n_voided value used for normalization\n",
    "            \"masks\": masks, #generated masks\n",
    "        }\n",
    "        return sample_dict \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726fd60b-77d1-49fb-9841-1ec55b2b502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef generate_white_matter_masks(segm, device, generator=None):\\n    n=segm.shape[0]\\n\\n    center = torch.empty(size=n)\\n    for i in torch.arange(n):\\n        #get indices with white matter\\n        wm_idx = torch.nonzero(segm[i])\\n        #take random white matter idx as center\\n        center[i] = torch.randint(low=0, high=wm_idx.shape[0], size=1, device=device, generator=generator)\\n\\n    low=3   \\n    high=40\\n    radius=torch.rand(n, device=device, generator=generator)*(high-low)+low # get radius between 3 and 40 from uniform distribution \\n\\n\\n    #samplen eines radius und schauen ob 70% white matter ist\\n    #falls nicht, dann radius um die hälfte verkleinern\\n    #wiederholen bis mindestgrösse erreicht und falls wieder nicht, dann neuer radius samplen\\n    \\n    \\n    \\n\\n    \\n    \\n    \\n\\n    #Test case\\n    #center=torch.tensor([[0,255],[0,255]]) \\n    #radius=torch.tensor([2,2])\\n    \\n    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\\n    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\\n    dist_from_center = dist_from_center.permute(2,0,1) \\n\\n    #Test case\\n    #print(dist_from_center[0,0,0]) #=255\\n    #print(dist_from_center[0,0,255]) #=360.624\\n    #print(dist_from_center[0,255,0]) #=0\\n    #print(dist_from_center[0,255,255]) #=255\\n    #print(dist_from_center[0,127,127]) #=180.313 \\n    \\n    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \\n    masks = masks[:,None,:,:].int() \\n    return masks\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def generate_white_matter_masks(segm, device, generator=None):\n",
    "    n=segm.shape[0]\n",
    "\n",
    "    center = torch.empty(size=n)\n",
    "    for i in torch.arange(n):\n",
    "        #get indices with white matter\n",
    "        wm_idx = torch.nonzero(segm[i])\n",
    "        #take random white matter idx as center\n",
    "        center[i] = torch.randint(low=0, high=wm_idx.shape[0], size=1, device=device, generator=generator)\n",
    "\n",
    "    low=3   \n",
    "    high=40\n",
    "    radius=torch.rand(n, device=device, generator=generator)*(high-low)+low # get radius between 3 and 40 from uniform distribution \n",
    "\n",
    "\n",
    "    #samplen eines radius und schauen ob 70% white matter ist\n",
    "    #falls nicht, dann radius um die hälfte verkleinern\n",
    "    #wiederholen bis mindestgrösse erreicht und falls wieder nicht, dann neuer radius samplen\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #Test case\n",
    "    #center=torch.tensor([[0,255],[0,255]]) \n",
    "    #radius=torch.tensor([2,2])\n",
    "    \n",
    "    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "    dist_from_center = dist_from_center.permute(2,0,1) \n",
    "\n",
    "    #Test case\n",
    "    #print(dist_from_center[0,0,0]) #=255\n",
    "    #print(dist_from_center[0,0,255]) #=360.624\n",
    "    #print(dist_from_center[0,255,0]) #=0\n",
    "    #print(dist_from_center[0,255,255]) #=255\n",
    "    #print(dist_from_center[0,127,127]) #=180.313 \n",
    "    \n",
    "    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \n",
    "    masks = masks[:,None,:,:].int() \n",
    "    return masks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94549ac197545158eebcc4e1b9da314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1915fa2a21948db911de8b04c3fcee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38455c393e074118a611c8cc466016f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n#create dataset\\ndatasetTrain = DatasetMRI(Path(config.dataset_train_path), Path(config.segm_train_path), pad_shape=(256, 256, 256), only_white_matter=True)\\ndatasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256))\\n\\nprint(f\"Dataset size: {len(datasetTrain)}\")\\nprint(f\"\\tImage shape: {datasetTrain[0][\\'gt_image\\'].shape}\")\\nprint(f\"Training Data: {list(datasetTrain[0].keys())}\") \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DatasetMRI2D import DatasetMRI2D\n",
    "from DatasetMRI3D import DatasetMRI3D\n",
    "from pathlib import Path\n",
    "\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI2D(Path(config.dataset_train_path), Path(config.segm_train_path), only_connected_masks=config.train_only_connected_masks)\n",
    "datasetEvaluation = DatasetMRI2D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "dataset3DEvaluation = DatasetMRI3D(Path(config.dataset_eval_path), Path(config.segm_eval_path), Path(config.masks_eval_path), only_connected_masks=config.eval_only_connected_masks)\n",
    "\n",
    "\"\"\"\n",
    "#create dataset\n",
    "datasetTrain = DatasetMRI(Path(config.dataset_train_path), Path(config.segm_train_path), pad_shape=(256, 256, 256), only_white_matter=True)\n",
    "datasetEvaluation = DatasetMRI(Path(config.dataset_eval_path), Path(config.segm_eval_path), pad_shape=(256, 256, 256))\n",
    "\n",
    "print(f\"Dataset size: {len(datasetTrain)}\")\n",
    "print(f\"\\tImage shape: {datasetTrain[0]['gt_image'].shape}\")\n",
    "print(f\"Training Data: {list(datasetTrain[0].keys())}\") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b10865-fac0-4151-a227-d426c2fd20da",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a6286c-155d-4d6c-923d-bef8ce0b180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x7fe5c3e14f40> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fe5c3e22a20> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axis = plt.subplots(1,2,figsize=(16,4))\n",
    "idx=80\n",
    "axis[0].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "axis[1].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "fig.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c57471-c0bf-4bca-9207-4dc6194bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 6 random sample\n",
    "random_indices = np.random.randint(0, len(datasetTrain) - 1, size=(6)) \n",
    "\n",
    "# Plot: t1n segmentations\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow(np.logical_or(datasetTrain[idx][\"segm\"].squeeze()==41, datasetTrain[idx][\"segm\"].squeeze()==2))\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6fe06-53ba-4056-bffb-c1c4acecacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: t1n images\n",
    "fig, axis = plt.subplots(2,3,figsize=(16,4))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axis[i//3,i%3].imshow((datasetTrain[idx][\"gt_image\"].squeeze()+1)/2)\n",
    "    axis[i//3,i%3].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba21352-9edc-4351-a359-de83bbed114d",
   "metadata": {},
   "source": [
    "### Playground for random circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18221bb-ac64-4876-9a87-24a8c0ff7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normal distributions of center points\n",
    "centers=[]\n",
    "for _ in np.arange(100):\n",
    "    centers.append(torch.normal(torch.tensor([127.,127.]),torch.tensor(30.)))\n",
    "\n",
    "plt.imshow((datasetTrain[70][\"gt_image\"].squeeze()+1)/2) \n",
    "for center in centers:\n",
    "    plt.scatter(center[0], center[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6508289-04f3-46c8-bc05-9629d7799e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=torch.zeros((10,256,256)).shape\n",
    "\n",
    "#create circular mask with random center around the center point of the pictures and a radius between 3 and 40 pixels\n",
    "center=np.random.normal([127,127],30, size=(example[0],2))\n",
    "radius=np.random.uniform(low=3,high=40, size=(example[0]))\n",
    "\n",
    "Y, X = np.ogrid[:256, :256] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "dist_from_center = np.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "\n",
    "mask = dist_from_center <= radius # creates mask for pixels which are inside the radius\n",
    "mask = 1-mask\n",
    "\n",
    "plt.imshow(((datasetTrain[70][\"gt_image\"].squeeze()+1)/2)*mask[:,:,4]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa2c12-25b0-41dd-a528-035b7c63f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ellips playground\n",
    "\"\"\"\n",
    "from math import pi\n",
    "\n",
    "mean=torch.normal(torch.tensor([127.,127.]),torch.tensor(30.)) # mean is sampled with normal distribution around the midpoint\n",
    "radius=np.random.uniform(low=3,high=50,size=(2)) # shape of ellipse is sampled with uniform distribution between 3 and 15\n",
    "\n",
    "u=mean[0]     #x-position of the center\n",
    "v=mean[1]    #y-position of the center\n",
    "a=radius[0]     #radius on the x-axis\n",
    "b=radius[1]    #radius on the y-axis\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "t = np.linspace(0, 2*pi, 100)\n",
    "plt.plot( u+a*np.cos(t) , v+b*np.sin(t),color=\"red\") \n",
    "plt.imshow((datasetTrain[500][\"gt_image\"].squeeze()+1)/2)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099f07-f1ff-46c1-bc83-2e448b80e778",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41941ef-16c4-4b3c-8e6f-712fee5b538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloader function, which is executed inside the training loop (necessary because of huggingface accelerate)\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509adcf-a0b1-4652-a5ca-b398b008dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3, # the number of input channels: 1 for img, 1 for img_voided, 1 for mask\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.model = \"UNet2DModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f67f7-b932-4f73-bdb3-96751ef6c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "#sample_image = datasetTrain[0]['gt_image'].unsqueeze(0)\n",
    "#noise = torch.randn(sample_image.shape)\n",
    "#timesteps = torch.LongTensor([50])\n",
    "#noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "#tb_summary.add_text(\"noise_scheduler\", \"DDIMScheduler(num_train_timesteps=1000)\", 0) \n",
    "\n",
    "#Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])\n",
    "\n",
    "\n",
    "config.noise_scheduler = \"DDIMScheduler(num_train_timesteps=1000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c3aa-3a69-4dfa-bbb2-d3c8086fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(datasetTrain)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "\n",
    "config.lr_scheduler = \"cosine_schedule_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4dfbe-cc66-4b78-8f36-6a5dae10abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from diffusers import DiffusionPipeline, DDIMScheduler, ImagePipelineOutput\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DDIMInpaintPipeline(DiffusionPipeline):\n",
    "    r\n",
    "    Pipeline for image inpainting.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet2DModel`]):\n",
    "            A `UNet2DModel` to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \n",
    "\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet, scheduler):\n",
    "        super().__init__()\n",
    "\n",
    "        # make sure scheduler can always be converted to DDIM\n",
    "        scheduler = DDIMScheduler.from_config(scheduler.config)\n",
    "\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        voided_imgs: torch.tensor,\n",
    "        masks: torch.tensor,\n",
    "        batch_size: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        eta: float = 0.0,\n",
    "        num_inference_steps: int = 50,\n",
    "        use_clipped_model_output: Optional[bool] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        r\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            voided_imgs ('torch.tensor'): \n",
    "                Images (1 channel) which should be inpainted.\n",
    "            masks ('torch.tensor'): \n",
    "                Binary masks which includes the area to inpaint.\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
    "                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers. A value of `0` corresponds to\n",
    "                DDIM and `1` corresponds to DDPM.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            use_clipped_model_output (`bool`, *optional*, defaults to `None`):\n",
    "                If `True` or `False`, see documentation for [`DDIMScheduler.step`]. If `None`, nothing is passed\n",
    "                downstream to the scheduler (use `None` for schedulers which don't support this argument).\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \n",
    "\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels-2, # Minus the two channels for the mask and the img to be inpainted\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (batch_size, self.unet.config.in_channels-2, *self.unet.config.sample_size) # Minus the two channels for the mask and the img to be inpainted\n",
    "\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        image = randn_tensor(image_shape, generator=generator, device=self._execution_device, dtype=self.unet.dtype)\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        #Input to unet model is concatenation of images, voided images and masks\n",
    "        input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(input, t).sample\n",
    "\n",
    "            # 2. predict previous mean of image x_t-1 and add variance depending on eta\n",
    "            # eta corresponds to η in paper and should be between [0, 1]\n",
    "            # do x_t -> x_t-1\n",
    "            image = self.scheduler.step(\n",
    "                model_output, t, image, eta=eta, use_clipped_model_output=use_clipped_model_output, generator=generator\n",
    "            ).prev_sample\n",
    "\n",
    "            #3. Concatenate image with voided images and masks\n",
    "            input=torch.cat((image, voided_imgs, masks), dim=1)\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return ImagePipelineOutput(images=image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b857ce-da16-4622-9c20-3d2572f208a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def generate_masks(n, device, generator=None):\n",
    "    #create circular mask with random center around the center point of the pictures and a radius between 3 and 50 pixels\n",
    "    center=torch.normal(mean=config.image_size/2, std=30, size=(n,2), generator=generator, device=device) # 30 is chosen by inspection\n",
    "    low=3\n",
    "    high=50\n",
    "    radius=torch.rand(n, device=device, generator=generator)*(high-low)+low # get radius between 3 and 50 from uniform distribution \n",
    "\n",
    "    #Test case\n",
    "    #center=torch.tensor([[0,255],[0,255]]) \n",
    "    #radius=torch.tensor([2,2])\n",
    "    \n",
    "    Y, X = [torch.arange(config.image_size, device=device)[:,None],torch.arange(config.image_size, device=device)[None,:]] # gives two vectors, each containing the pixel locations. There's a column vector for the column indices and a row vector for the row indices.\n",
    "    dist_from_center = torch.sqrt((X.T - center[:,0])[None,:,:]**2 + (Y-center[:,1])[:,None,:]**2) # creates matrix with euclidean distance to center\n",
    "    dist_from_center = dist_from_center.permute(2,0,1) \n",
    "\n",
    "    #Test case\n",
    "    #print(dist_from_center[0,0,0]) #=255\n",
    "    #print(dist_from_center[0,0,255]) #=360.624\n",
    "    #print(dist_from_center[0,255,0]) #=0\n",
    "    #print(dist_from_center[0,255,255]) #=255\n",
    "    #print(dist_from_center[0,127,127]) #=180.313 \n",
    "    \n",
    "    masks = dist_from_center > radius[:,None,None] # creates mask for pixels which are outside the radius. \n",
    "    masks = masks[:,None,:,:].int() \n",
    "    return masks\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5efb3d-3fc5-456b-afc8-40220ff5a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#setup evaluation\n",
    "from diffusers.utils import make_image_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline, eval_dataloader):\n",
    "    batch = next(iter(eval_dataloader)) #TODO: Anpassen, falls grösseres evaluation set. Evt. anpassen für accelerate.\n",
    "    clean_images = batch[\"gt_image\"]\n",
    "\n",
    "    generator = torch.cuda.manual_seed_all(config.seed)\n",
    "    masks = generate_masks(n=clean_images.shape[0], generator=generator, device=clean_images.device)\n",
    "    voided_images = clean_images*masks\n",
    "\n",
    "    images = pipeline(\n",
    "        voided_images,\n",
    "        masks,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed_all(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=int(config.eval_batch_size**0.5), cols=int(config.eval_batch_size**0.5))\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/inpainted_image_{epoch:04d}.png\")\n",
    "    \n",
    "    pil_voided_images = [to_pil_image((x+1)/2) for x in voided_images]\n",
    "    voided_image_grid = make_image_grid(pil_voided_images, rows=int(config.eval_batch_size**0.5), cols=int(config.eval_batch_size**0.5))\n",
    "    voided_image_grid.save(f\"{test_dir}/voided_image_{epoch:04d}.png\")\n",
    "    \n",
    "    print(\"image saved\")\n",
    "\n",
    "#TODO: As soon as I evaluate metrics I need to adjust the evaluate function to accelerate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6078de-93ba-4c23-8f45-6b2e737cd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#from accelerate import Accelerator\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, lr_scheduler):\n",
    "    # setup training environment\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"tensorboard\"),\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True) \n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    train_dataloader = get_dataloader(dataset = datasetTrain, batch_size = config.train_batch_size, shuffle=True) \n",
    "    eval_dataloader = get_dataloader(dataset = datasetEvaluation, batch_size = config.eval_batch_size, shuffle=False) \n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)  \n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #eval test case\n",
    "    #epoch=0\n",
    "    #model.eval()\n",
    "    #pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)  \n",
    "    #evaluate(config, epoch, pipeline, eval_dataloader)\n",
    "    \n",
    "    \n",
    "    # Train model\n",
    "    model.train()\n",
    "    for epoch in range(config.num_epochs): \n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process) \n",
    "        progress_bar.set_description(f\"Epoch {epoch}\") \n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            \n",
    "            clean_images = batch[\"gt_image\"]\n",
    "            segmentations = batch[\"segm\"]\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            #create mask for each img\n",
    "            masks = generate_masks(n = clean_images.shape[0], device=clean_images.device)\n",
    "            \n",
    "            #create voided img\n",
    "            voided_images = clean_images*masks\n",
    "\n",
    "            # Add noise to the voided images according to the noise magnitude at each timestep (forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) \n",
    "\n",
    "            # concatenate noisy_images, voided_images and mask\n",
    "            input=torch.cat((noisy_images, voided_images, masks), dim=1)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(input, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "                #log gradient norm \n",
    "                parameters = [p for p in model.parameters() if p.grad is not None and p.requires_grad]\n",
    "                if len(parameters) == 0:\n",
    "                    total_norm = 0.0\n",
    "                else: \n",
    "                    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()).cpu() for p in parameters]), 2.0).item()\n",
    "\n",
    "                #do learning step\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # save logs\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {\"loss\": loss.cpu().detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"total_norm\": total_norm, \"step\": global_step}\n",
    "                tb_summary.add_scalar(\"loss\", logs[\"loss\"], global_step)\n",
    "                tb_summary.add_scalar(\"lr\", logs[\"lr\"], global_step) \n",
    "                tb_summary.add_scalar(\"total_norm\", logs[\"total_norm\"], global_step) \n",
    "            \n",
    "                progress_bar.set_postfix(**logs)\n",
    "                #accelerator.log(logs, step=global_step)\n",
    "            global_step += 1 \n",
    "\n",
    "        # After a certain number of epochs it samples some images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            model.eval()\n",
    "            pipeline = DDIMInpaintPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler) \n",
    "    \n",
    "            if (epoch) % config.evaluate_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline, eval_dataloader)\n",
    "    \n",
    "            if (epoch) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1: \n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "tb_summary.add_text(\"inference_pipeline\", \"DDIMPipeline\", 0) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f42e5-88c7-4298-8ad7-2b7b4555a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from accelerate import notebook_launcher\n",
    "\n",
    "# If run from a jupyter notebook then uncomment the two lines and remove last line\n",
    "args = (config, model, noise_scheduler, optimizer, lr_scheduler)\n",
    "#notebook_launcher(train_loop, args, num_processes=config.num_gpu)\n",
    "#notebook_launcher(train_loop, args, num_processes=1)\n",
    "\n",
    "train_loop(config, model, noise_scheduler, optimizer, lr_scheduler)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b1603-390b-48b7-81fc-6d0bd96dbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingConditional import TrainingConditional\n",
    "from DDIMInpaintPipeline import DDIMInpaintPipeline\n",
    "\n",
    "config.conditional_data = \"Circles\"\n",
    "\n",
    "args = {\"config\": config, \"model\": model, \"noise_scheduler\": noise_scheduler, \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"datasetTrain\": datasetTrain, \"datasetEvaluation\": datasetEvaluation, \"dataset3DEvaluation\": dataset3DEvaluation, \"trainingCircularMasks\": True} \n",
    "trainingCircles = TrainingConditional(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4044c2-9483-4457-bd53-51c795c7bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"train\":\n",
    "    trainingCircles.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9106d1-bf7b-4ec0-bd4c-e980051e7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"eval\":\n",
    "    pipeline = DDIMInpaintPipeline.from_pretrained(config.output_dir) \n",
    "    trainingCircles.evaluate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f790341-a14e-44fe-bfce-a88dec97707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf38b5e5-8e55-4e55-b756-4bfd75248e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lesion_filling_conditioned_circles.ipynb to script\n",
      "[NbConvertApp] Writing 32261 bytes to lesion_filling_conditioned_circles.py\n"
     ]
    }
   ],
   "source": [
    "#create python script for ubelix\n",
    "!jupyter nbconvert --to script \"lesion_filling_conditioned_circles.ipynb\"\n",
    "filename=\"lesion_filling_conditioned_circles.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606a4de-dcd7-47d3-bbdc-b0d6491d6439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8eff5-2d96-4ec1-aed0-a97408f7fb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
