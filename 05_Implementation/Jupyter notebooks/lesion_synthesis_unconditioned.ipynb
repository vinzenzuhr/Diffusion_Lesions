{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f1e0a8-8f60-4b1b-94b8-8367832dccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config: \n",
    "    target_shape = None # will transform t1n during preprocessing (computationally expensive)\n",
    "    unet_img_shape = (160,256)\n",
    "    channels = 1\n",
    "    effective_train_batch_size=16 \n",
    "    eval_batch_size = 4   \n",
    "    sorted_slice_sample_size = 1\n",
    "    num_dataloader_workers = 8\n",
    "    num_epochs = 192\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    evaluate_epochs = 5 # adjust to num_epochs\n",
    "    evaluate_num_batches = -1 # ~3s/batch. 2.5 min/Evaluation 3D epoch with all batchesr\n",
    "    evaluate_num_batches_3d = -1 \n",
    "    evaluate_unconditional_img = True\n",
    "    deactivate2Devaluation = False\n",
    "    deactivate3Devaluation = True\n",
    "    evaluate_3D_epochs = 1000  # 3 min/Evaluation 3D \n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"lesion-synthesis-256-unconditioned\"  # the model name locally and on the HF Hub\n",
    "    dataset_train_path = \"./datasets/synthesis/dataset_train/imgs\"\n",
    "    segm_train_path = \"./datasets/synthesis/dataset_train/segm\"\n",
    "    masks_train_path = \"./datasets/synthesis/dataset_train/masks\"\n",
    "    dataset_eval_path = \"./datasets/synthesis/dataset_eval/imgs\"\n",
    "    segm_eval_path = \"./datasets/synthesis/dataset_eval/segm\"\n",
    "    masks_eval_path = \"./datasets/synthesis/dataset_eval/masks\"\n",
    "    train_connected_masks=False  # No Training with lesion masks\n",
    "    eval_connected_masks=False \n",
    "    num_inference_steps=50\n",
    "    log_csv = True\n",
    "    add_lesion_technique = \"other_lesions_99Quantile\" # 'mean_intensity', 'other_lesions_1stQuantile', 'other_lesions_mean', 'other_lesions_median', 'other_lesions_3rdQuantile', 'other_lesions_99Quantile' \n",
    "    intermediate_timestep = 3 # starting from this timesteps. num_inference_steps means the whole pipeline and 1 the last step. \n",
    "    mode = \"train\" # 'train', 'eval' or \"tuning_parameters\"\n",
    "    debug = True     \n",
    "    jump_length=intermediate_timestep\n",
    "    jump_n_sample=3\n",
    "    brightness_augmentation = False\n",
    "    eval_loss_timesteps=[20,40,80,140]\n",
    "    restrict_train_slices = \"segm\"\n",
    "    restrict_eval_slices = \"mask\"\n",
    "    use_min_snr_loss=True\n",
    "    snr_gamma=5\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub \n",
    "    seed = 0\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1f4a9f-d91e-479c-ae33-8fe3b58d992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jovyan/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup huggingface accelerate\n",
    "import torch\n",
    "import numpy as np\n",
    "import accelerate\n",
    "accelerate.commands.config.default.write_basic_config(config.mixed_precision)\n",
    "#if there are problems with ports then add manually \"main_process_port: 0\" or another number to yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ef574b-c7e4-4588-b194-cb41cfda8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "with open(Path.home() / \".cache/huggingface/accelerate/default_config.yaml\") as f:\n",
    "    data = json.load(f)\n",
    "    config.num_processes = data[\"num_processes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b28bb93-5c54-42e5-ac9b-38c26a3a96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_batch_size = int((config.effective_train_batch_size / config.gradient_accumulation_steps) / config.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc472c2-c374-4859-8ed5-791ebeafbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    config.num_inference_steps = 1\n",
    "    config.intermediate_timestep = 1\n",
    "    config.train_batch_size = 1\n",
    "    config.eval_batch_size = 1\n",
    "    config.eval_loss_timesteps = [20]\n",
    "    config.train_connected_masks=False\n",
    "    config.eval_connected_masks=False\n",
    "    config.evaluate_num_batches = 3\n",
    "    config.deactivate3Devaluation = False\n",
    "    config.dataset_train_path = \"./datasets/synthesis/dataset_eval/imgs\"\n",
    "    config.segm_train_path = \"./datasets/synthesis/dataset_eval/segm\"\n",
    "    config.masks_train_path = \"./datasets/synthesis/dataset_eval/masks\" \n",
    "    config.num_dataloader_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c99f6d3-517a-48ab-a8ff-7c8e869080af",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(config.eval_loss_timesteps) == config.eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2b7df9-0219-4c4e-99f2-08c9b1fc2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:58:20.804144: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 11:58:21.709384: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-14 11:58:21.709448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-14 11:58:21.709504: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-14 11:58:21.764558: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 11:58:21.783251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 11:58:40.567857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d4ba6f7d1b4841b8fced83d7b0ba34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2854fdd7e9469e9aa723c3f9707d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53506d8f894c06a1c64844a6d04aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from custom_modules import DatasetMRI2D, DatasetMRI3D, ScaleDecorator\n",
    "from pathlib import Path\n",
    "from torchvision import transforms \n",
    "\n",
    "#add augmentation\n",
    "transformations = None\n",
    "if config.brightness_augmentation:\n",
    "    transformations = transforms.RandomApply([ScaleDecorator(transforms.ColorJitter(brightness=1))], p=0.5)\n",
    "\n",
    "#create dataset\n",
    "dataset_train = DatasetMRI2D(root_dir_img=Path(config.dataset_train_path), restriction=config.restrict_train_slices, root_dir_segm=Path(config.segm_train_path), connected_masks=config.train_connected_masks, target_shape=config.target_shape, transforms=transformations)\n",
    "dataset_evaluation = DatasetMRI2D(root_dir_img=Path(config.dataset_eval_path), restriction=config.restrict_eval_slices, root_dir_masks=Path(config.masks_eval_path), root_dir_synthesis=Path(config.masks_eval_path), connected_masks=config.eval_connected_masks, target_shape=config.target_shape)\n",
    "dataset_3D_evaluation = DatasetMRI3D(root_dir_img=Path(config.dataset_eval_path), root_dir_masks=Path(config.masks_eval_path), root_dir_synthesis=Path(config.masks_eval_path), connected_masks=config.eval_connected_masks, target_shape=config.target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885175f-4240-49fd-b199-061a3c5ea94e",
   "metadata": {},
   "source": [
    "### Finding good intensity of lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a84b00f-2c7b-4583-b9e3-91238db9e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "from tqdm.auto import tqdm \n",
    "from custom_modules import DatasetMRI2D, DatasetMRI3D, ScaleDecorator\n",
    "\n",
    "#skip to speed up\n",
    "if False:\n",
    "    #t1w_norm_noskull_list = list(Path(\"temp/unhealthy_DL+DiReCT_Segmentation\").rglob(\"*T1w_norm_noskull.nii.gz\"))\n",
    "    #lesion_list = list(Path(\"temp/unhealthy_registered_lesions\").rglob(\"*transformed_lesion.nii.gz\"))\n",
    "    #t1w_norm_noskull_list = list(Path(\"datasets_full/unhealthy_flair\").rglob(\"*3DFLAIR.nii.gz\"))\n",
    "    #lesion_list = list(Path(\"datasets_full/unhealthy_flair_masks\").rglob(\"*Consensus.nii.gz\"))\n",
    "\n",
    "    dataset_eval_path=\"datasets_full/unhealthy_flair\"\n",
    "    masks_eval_path=\"datasets_full/unhealthy_flair_masks\"\n",
    "    \n",
    "    dataset_3D_evaluation = DatasetMRI3D(root_dir_img=Path(dataset_eval_path), root_dir_masks=Path(masks_eval_path), root_dir_synthesis=Path(masks_eval_path), only_connected_masks=config.eval_only_connected_masks, t1n_target_shape=config.t1n_target_shape)\n",
    "    \n",
    "    t1ws = list()\n",
    "    lesions = list()\n",
    "    for i in tqdm(range(len(dataset_3D_evaluation))):\n",
    "        t1w = dataset_3D_evaluation[i][\"gt_image\"]\n",
    "        lesion = dataset_3D_evaluation[i][\"mask\"]\n",
    "        \n",
    "        #t1w = t1w.get_fdata()\n",
    "        #t1w, _, lesion, _, _ = DatasetMRI3D.preprocess(t1w, masks=lesion)\n",
    "        #lesion = lesion.get_fdata()\n",
    "        #lesion = DatasetMRI3D._padding(torch.from_numpy(lesion).to(torch.uint8))\n",
    "        #means.append(t1w[lesion.to(torch.bool)].mean())\n",
    "        #stds.append(t1w[lesion.to(torch.bool)].std())\n",
    "        t1ws.append(t1w)\n",
    "        lesions.append(lesion)\n",
    "    t1w_big = torch.cat(t1ws)\n",
    "    lesion_big = torch.cat(lesions)\n",
    "    \n",
    "    print(\"mean lesion intensity: \", t1w_big[lesion_big.to(torch.bool)].mean()) # -0.3572\n",
    "    print(\"std lesion intensity: \", t1w_big[lesion_big.to(torch.bool)].std()) # 0.1829\n",
    "    print(\"median lesion intensity: \", t1w_big[lesion_big.to(torch.bool)].median()) # 0.1829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ce44b1-5aa0-4760-b79f-0dab3be0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    " \n",
    "\n",
    "#skip to speed up\n",
    "if False: \n",
    "    random_idx = torch.randint(len(dataset_evaluation)-1, size=(1,)).item()\n",
    "    \n",
    "    fig, axis = plt.subplots(1,4, figsize=(16,4)) \n",
    "    img = dataset_evaluation[random_idx][\"gt_image\"].squeeze()\n",
    "    img[dataset_evaluation[random_idx][\"mask\"].to(torch.bool).squeeze()] = -0.5492 # mean-std of lesion intensity\n",
    "    axis[0].imshow(dataset_evaluation[random_idx][\"gt_image\"].squeeze()/2+0.5)\n",
    "    axis[0].set_axis_off()\n",
    "    axis[1].imshow(dataset_evaluation[random_idx][\"mask\"].squeeze())\n",
    "    axis[1].set_axis_off()\n",
    "    axis[2].imshow(img/2+0.5)\n",
    "    axis[2].set_axis_off()\n",
    "    axis[3].imshow(dataset_evaluation[random_idx][\"synthesis\"].squeeze())\n",
    "    axis[3].set_axis_off()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c744a-8c9f-42fb-9342-f7735c5b5032",
   "metadata": {},
   "source": [
    "### Playground LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da88e984-791b-4fc7-8f3a-61ea3dd1d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip to speed up\n",
    "if False:\n",
    "    from Evaluation2DSynthesis import Evaluation2DSynthesis\n",
    "    from accelerate import Accelerator\n",
    "    from DDIMGuidedPipeline import DDIMGuidedPipeline\n",
    "    \n",
    "    pipeline = DDIMGuidedPipeline.from_pretrained(config.output_dir) \n",
    "    \n",
    "    eval = Evaluation2DSynthesis(config, pipeline, None, None, Accelerator(), None)\n",
    "    images_with_lesions, _ = eval._add_coarse_lesions(dataset_evaluation[123][\"gt_image\"], dataset_evaluation[123])\n",
    "    clean_image=dataset_evaluation[123][\"gt_image\"]\n",
    "    synthesized_images = pipeline(images_with_lesions, 1).images\n",
    "    synthesized_images_2 = pipeline(images_with_lesions, 3).images\n",
    "    print(\"Pips for one timestep: \", eval._calc_lpip(clean_image.unsqueeze(0), synthesized_images))\n",
    "    print(\"Pips for three timestep: \", eval._calc_lpip(clean_image.unsqueeze(0), synthesized_images_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d4edd7-5123-42f2-8a98-5b4c1c464164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip to speed up\n",
    "if False:\n",
    "    fig, axis = plt.subplots(1,4, figsize=(24,12))\n",
    "    axis[0].imshow(clean_image.squeeze()/2+0.5)\n",
    "    axis[0].set_title(\"original image\")\n",
    "    axis[1].imshow(images_with_lesions.squeeze()/2+0.5)\n",
    "    axis[1].set_title(\"coarse lesions\")\n",
    "    axis[2].imshow(synthesized_images.squeeze()/2+0.5)\n",
    "    axis[2].set_title(\"Diffusion with one timestep\")\n",
    "    axis[3].imshow(synthesized_images_2.squeeze()/2+0.5)\n",
    "    axis[3].set_title(\"Diffusion with three timestep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a086dd-5273-4c44-aa62-e9cd98a0c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip to speed up\n",
    "if False:\n",
    "    diff_1=clean_image-synthesized_images\n",
    "    diff_3=clean_image-synthesized_images_2\n",
    "    fig, axis = plt.subplots(1,2, figsize=(16,8))\n",
    "    axis[0].imshow(diff_1.squeeze()/2+0.5)\n",
    "    axis[0].set_title(\"Differences with one timestep\")\n",
    "    axis[1].imshow(diff_3.squeeze()/2+0.5)\n",
    "    axis[1].set_title(\"Differences with three timestep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799bdb3-4c83-4ab8-8e46-33d2fe30f49d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1188218d-a16d-4c0a-896f-e47a140bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.unet_img_shape,  # the target image resolution\n",
    "    in_channels=config.channels,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=config.channels,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.model = \"UNet2DModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4a40c63-4ba1-44aa-8c62-7f15ce816225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup noise scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "config.noise_scheduler = \"DDIMScheduler(num_train_timesteps=1000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f42d0a-9ebb-4437-b180-12b79af3c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lr scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(math.ceil(len(dataset_train)/config.train_batch_size) * config.num_epochs), # num_iterations per epoch * num_epochs\n",
    ")\n",
    "config.lr_scheduler = \"cosine_schedule_with_warmup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd128d1f-5ef7-40a4-8c53-30a774dfa18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator \n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950d51f7-8721-43e7-8ea7-5d4e06db6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if config.output_dir is not None:\n",
    "        os.makedirs(config.output_dir, exist_ok=True) \n",
    "    #setup tensorboard\n",
    "    tb_summary = SummaryWriter(config.output_dir, purge_step=0)\n",
    "    accelerator.init_trackers(\"train_example\") #maybe delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc7565cf-5075-4eae-afcf-1fc7b45bf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process:\n",
    "    from custom_modules import Logger\n",
    "    logger = Logger(tb_summary, config.log_csv)\n",
    "    logger.log_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a27ccd0a-4b63-40ac-83f5-d7c1a9cfa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modules import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(dataset=dataset_train, batch_size = config.train_batch_size, \n",
    "                                  num_workers=config.num_dataloader_workers, random_sampler=True, \n",
    "                                  seed=config.seed, multi_slice=config.sorted_slice_sample_size > 1)\n",
    "d2_eval_dataloader = get_dataloader(dataset=dataset_evaluation, batch_size = config.eval_batch_size, \n",
    "                                    num_workers=config.num_dataloader_workers, random_sampler=False, \n",
    "                                    seed=config.seed, multi_slice=config.sorted_slice_sample_size > 1)\n",
    "d3_eval_dataloader = get_dataloader(dataset=dataset_3D_evaluation, batch_size = 1, \n",
    "                                    num_workers=config.num_dataloader_workers, random_sampler=False, \n",
    "                                    seed=config.seed, multi_slice=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "223ab8a9-93b1-4268-acfc-dc484e5ef8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modules import ModelInputGenerator, Evaluation2DSynthesis, Evaluation3DSynthesis\n",
    "\n",
    "model_input_generator = ModelInputGenerator(conditional=False, noise_scheduler=noise_scheduler)\n",
    "\n",
    "args = {\n",
    "    \"intermediate_timestep\": config.intermediate_timestep,\n",
    "    \"add_lesion_technique\": config.add_lesion_technique,\n",
    "    \"eval_dataloader\": d2_eval_dataloader, \n",
    "    \"train_dataloader\": train_dataloader,\n",
    "    \"logger\": None if not accelerator.is_main_process else logger, \n",
    "    \"accelerator\": accelerator,\n",
    "    \"num_inference_steps\": config.num_inference_steps,\n",
    "    \"model_input_generator\": model_input_generator,\n",
    "    \"output_dir\": config.output_dir,\n",
    "    \"eval_loss_timesteps\": config.eval_loss_timesteps, \n",
    "    \"evaluate_num_batches\": config.evaluate_num_batches, \n",
    "    \"seed\": config.seed\n",
    "}\n",
    "evaluation2D = Evaluation2DSynthesis(**args)\n",
    "args = {\n",
    "    \"intermediate_timestep\": config.intermediate_timestep,\n",
    "    \"add_lesion_technique\": config.add_lesion_technique,\n",
    "    \"dataloader\": d3_eval_dataloader, \n",
    "    \"logger\": None if not accelerator.is_main_process else logger, \n",
    "    \"accelerator\": accelerator,\n",
    "    \"output_dir\": config.output_dir,\n",
    "    \"num_inference_steps\": config.num_inference_steps,\n",
    "    \"eval_batch_size\": config.eval_batch_size,\n",
    "    \"sorted_slice_sample_size\": config.sorted_slice_sample_size,\n",
    "    \"evaluate_num_batches\": config.evaluate_num_batches_3d,\n",
    "    \"seed\": config.seed,\n",
    "}\n",
    "evaluation3D = Evaluation3DSynthesis(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ff284a-4306-4677-829e-98fef153fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modules import Training, GuidedRePaintPipeline, Evaluation2DSynthesis, Evaluation3DSynthesis \n",
    "from custom_modules import PipelineFactories\n",
    "\n",
    "config.conditional_data = \"None\"\n",
    "\n",
    "args = { \n",
    "    \"accelerator\": accelerator,\n",
    "    \"model\": model, \n",
    "    \"noise_scheduler\": noise_scheduler, \n",
    "    \"optimizer\": optimizer, \n",
    "    \"lr_scheduler\": lr_scheduler, \n",
    "    \"train_dataloader\": train_dataloader, \n",
    "    \"d2_eval_dataloader\": d2_eval_dataloader, \n",
    "    \"d3_eval_dataloader\": d3_eval_dataloader, \n",
    "    \"model_input_generator\": model_input_generator,\n",
    "    \"evaluation2D\": evaluation2D,\n",
    "    \"evaluation3D\": evaluation3D,\n",
    "    \"logger\": None if not accelerator.is_main_process else logger,\n",
    "    \"pipeline_factory\": PipelineFactories.get_guided_repaint_pipeline,\n",
    "    \"num_epochs\": config.num_epochs, \n",
    "    \"evaluate_2D_epochs\": config.evaluate_epochs,\n",
    "    \"evaluate_3D_epochs\": config.evaluate_3D_epochs,\n",
    "    \"min_snr_loss\": config.use_min_snr_loss,\n",
    "    \"snr_gamma\": config.snr_gamma,\n",
    "    \"evaluate_unconditional_img\": config.evaluate_unconditional_img,\n",
    "    \"deactivate_2D_evaluation\": config.deactivate2Devaluation, \n",
    "    \"deactivate_3D_evaluation\": config.deactivate3Devaluation, \n",
    "    \"evaluation_pipeline_parameters\": {\n",
    "        \"jump_length\": config.jump_length,\n",
    "        \"jump_n_sample\": config.jump_n_sample,\n",
    "    },\n",
    "    \"debug\": config.debug, \n",
    "    }\n",
    "trainingSynthesis = Training(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac4e0a-2837-4217-a97c-ce940bdc2151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb8a3692e664950aacb771147041c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.mode == \"train\":\n",
    "    trainingSynthesis.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7971-c0ac-461d-9fcf-78481e7c9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mode == \"eval\":\n",
    "    trainingSynthesis.deactivate_3D_evaluation = False\n",
    "    pipeline = GuidedRePaintPipeline.from_pretrained(config.output_dir) \n",
    "    trainingSynthesis.evaluate(pipeline, deactivate_save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e6ddc-f89d-451d-9a79-4edc40c05230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if config.mode == \"tuning_parameters\":\n",
    "    trainingSynthesis.deactivate_3D_evaluation = False\n",
    "    pipeline = GuidedRePaintPipeline.from_pretrained(config.output_dir)\n",
    "    original_output_dir = config.output_dir \n",
    "    timesteps = [1, 3, 5]\n",
    "    resample_step = [1, 3, 5]\n",
    "    parameters = product(timesteps, resample_step) \n",
    "    \n",
    "    for timestep, resample_step in parameters:\n",
    "        print(\"Begin evaluation for timestep \", timestep, \" and resample step \", resample_step)\n",
    "        trainingSynthesis.intermediate_timestep = timestep\n",
    "        trainingSynthesis.jump_length = timestep\n",
    "        trainingSynthesis.jump_n_sample = resample_step\n",
    "        trainingSynthesis.output_dir = original_output_dir + \"/tuning_parameters/timestep_\" + str(timestep) + \"_resample_\" + str(resample_step)\n",
    "        if trainingSynthesis.accelerator.is_main_process:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "            trainingSynthesis.log_meta_logs()\n",
    "        trainingSynthesis.evaluate(pipeline, deactivate_save_model=True) \n",
    "\n",
    "    # plot lpips score vs  \n",
    "    folder_list = list(Path().glob(original_output_dir + \"/tuning_parameters/timestep_*\"))\n",
    "    lpips = []\n",
    "    labels = [] \n",
    "    for folder in folder_list:\n",
    "        timestep = str(folder).split(\"_\")[-3] \n",
    "        resample_step = str(folder).split(\"_\")[-1]\n",
    "        with open(folder / \"metrics.csv\", 'r') as fp: \n",
    "            _ = fp.readline()\n",
    "            csv_metrics = fp.readline()\n",
    "            reader = csv_metrics.split(',') \n",
    "            for metric in reader:\n",
    "                if metric != \"\":\n",
    "                    name, value = metric.split(':')\n",
    "                    if name == \"lpips\":\n",
    "                        lpips.append(float(value))\n",
    "                        labels.append(timestep + \"_\" + resample_step) \n",
    "    plt.clf()\n",
    "    plt.bar(labels, lpips) \n",
    "    plt.savefig(original_output_dir + '/lpips_parameters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be170a32-fcb6-41b1-b6ea-14b77eb7b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create python script for ubelix \n",
    "import os\n",
    "\n",
    "!jupyter nbconvert --to script \"lesion_synthesis_unconditioned.ipynb\"\n",
    "filename=\"lesion_synthesis_unconditioned.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
